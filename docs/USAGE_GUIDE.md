# Context Scraper MCP Server - ä½¿ç”¨æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å¯åŠ¨æœåŠ¡å™¨
```bash
python server.py
```

### åœ¨ Amazon Q ä¸­é…ç½®
```json
{
  "mcpServers": {
    "context-scraper": {
      "command": "python",
      "args": ["/path/to/context-scraper-mcp-server/server.py"]
    }
  }
}
```

## ğŸ”§ åŠŸèƒ½è¯¦è§£

### 1. åŸºç¡€çˆ¬å– - `crawl(url)`
**ç”¨é€”**: çˆ¬å–å•ä¸ªç½‘é¡µï¼Œè¿”å› Markdown æ ¼å¼å†…å®¹
**ç¤ºä¾‹**:
```
crawl("https://docs.crawl4ai.com")
```

### 2. é€‰æ‹©å™¨çˆ¬å– - `crawl_with_selector(url, css_selector)`
**ç”¨é€”**: ä½¿ç”¨ CSS é€‰æ‹©å™¨ç²¾ç¡®æå–ç‰¹å®šå†…å®¹
**ç¤ºä¾‹**:
```
# åªæå–æ–‡ç« å†…å®¹
crawl_with_selector("https://blog.example.com/post", "article")

# æå–æ‰€æœ‰äº§å“å¡ç‰‡
crawl_with_selector("https://shop.example.com", ".product-card")

# æå–å¯¼èˆªé“¾æ¥
crawl_with_selector("https://example.com", "nav a")
```

**å¸¸ç”¨é€‰æ‹©å™¨**:
- `article` - æ–‡ç« å†…å®¹
- `.content, .main-content` - ä¸»è¦å†…å®¹åŒºåŸŸ
- `h1, h2, h3` - æ ‡é¢˜
- `.product, .item` - äº§å“ä¿¡æ¯
- `nav a` - å¯¼èˆªé“¾æ¥
- `.post, .blog-post` - åšå®¢æ–‡ç« 

### 3. æ‰¹é‡çˆ¬å– - `crawl_multiple(urls_str)`
**ç”¨é€”**: åŒæ—¶çˆ¬å–å¤šä¸ª URLï¼Œç”¨é€—å·åˆ†éš”
**ç¤ºä¾‹**:
```
crawl_multiple("https://site1.com,https://site2.com,https://site3.com")
```

### 4. å¥åº·æ£€æŸ¥ - `health_check(url)`
**ç”¨é€”**: æ£€æŸ¥ç½‘ç«™æ˜¯å¦å¯è®¿é—®
**ç¤ºä¾‹**:
```
health_check("https://example.com")
```

## ğŸ“ æç¤ºåŠŸèƒ½

### 1. åˆ›å»ºä¸Šä¸‹æ–‡ - `create_context_from_url(url)`
**ç”¨é€”**: å°†ç½‘é¡µå†…å®¹ä¿å­˜åˆ° Amazon Q çš„ä¸Šä¸‹æ–‡è§„åˆ™ä¸­
**ç¤ºä¾‹**:
```
create_context_from_url("https://docs.crawl4ai.com/installation")
```

### 2. ç ”ç©¶æ–‡æ¡£ - `create_research_context(topic, urls)`
**ç”¨é€”**: ä»å¤šä¸ª URL åˆ›å»ºç ”ç©¶æ–‡æ¡£
**ç¤ºä¾‹**:
```
create_research_context("AI Web Scraping", "https://crawl4ai.com,https://docs.crawl4ai.com")
```

### 3. ç‰¹å®šå†…å®¹æå– - `extract_specific_content(url, content_type)`
**ç”¨é€”**: æå–ç‰¹å®šç±»å‹çš„å†…å®¹
**ç¤ºä¾‹**:
```
extract_specific_content("https://news.site.com", "articles")
extract_specific_content("https://shop.site.com", "products")
```

**æ”¯æŒçš„å†…å®¹ç±»å‹**:
- `articles` - æ–‡ç« å†…å®¹
- `products` - äº§å“ä¿¡æ¯
- `links` - é“¾æ¥
- `headings` - æ ‡é¢˜
- `images` - å›¾ç‰‡
- `navigation` - å¯¼èˆª
- `forms` - è¡¨å•
- `tables` - è¡¨æ ¼

## ğŸ’¡ å®ç”¨åœºæ™¯

### åœºæ™¯ 1: æŠ€æœ¯æ–‡æ¡£æ•´ç†
```
# çˆ¬å–å¤šä¸ªæ–‡æ¡£é¡µé¢
crawl_multiple("https://docs.example.com/guide1,https://docs.example.com/guide2,https://docs.example.com/guide3")

# æˆ–ä½¿ç”¨ç ”ç©¶æ–‡æ¡£åŠŸèƒ½
create_research_context("API Documentation", "https://docs.example.com/api,https://docs.example.com/sdk")
```

### åœºæ™¯ 2: ç«å“åˆ†æ
```
# æå–äº§å“ä¿¡æ¯
extract_specific_content("https://competitor.com/products", "products")

# å¥åº·æ£€æŸ¥ç«å“ç½‘ç«™
health_check("https://competitor.com")
```

### åœºæ™¯ 3: æ–°é—»èšåˆ
```
# åªæå–æ–°é—»æ–‡ç« å†…å®¹
crawl_with_selector("https://news.site.com/article", "article")

# æ‰¹é‡çˆ¬å–å¤šä¸ªæ–°é—»æº
crawl_multiple("https://news1.com/today,https://news2.com/today,https://news3.com/today")
```

### åœºæ™¯ 4: å­¦æœ¯ç ”ç©¶
```
# åˆ›å»ºç ”ç©¶ä¸»é¢˜æ–‡æ¡£
create_research_context("Machine Learning", "https://arxiv.org/paper1,https://arxiv.org/paper2")

# æå–è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦
crawl_with_selector("https://arxiv.org/abs/2301.00001", ".title, .abstract")
```

## ğŸ”§ é«˜çº§æŠ€å·§

### 1. CSS é€‰æ‹©å™¨ç»„åˆ
```
# å¤šä¸ªé€‰æ‹©å™¨
crawl_with_selector("https://example.com", "h1, h2, .important")

# å±‚çº§é€‰æ‹©å™¨
crawl_with_selector("https://example.com", ".content article p")

# å±æ€§é€‰æ‹©å™¨
crawl_with_selector("https://example.com", "a[href*='download']")
```

### 2. æ‰¹é‡å¤„ç†ä¼˜åŒ–
```
# ç›¸å…³é¡µé¢ä¸€èµ·å¤„ç†ï¼ˆæ›´é«˜æ•ˆï¼‰
crawl_multiple("https://docs.site.com/page1,https://docs.site.com/page2")

# ä¸åŒåŸŸååˆ†å¼€å¤„ç†
crawl_multiple("https://site1.com/page1,https://site1.com/page2")
crawl_multiple("https://site2.com/page1,https://site2.com/page2")
```

### 3. é”™è¯¯å¤„ç†
```
# å…ˆæ£€æŸ¥ç½‘ç«™å¯ç”¨æ€§
health_check("https://example.com")

# å†è¿›è¡Œçˆ¬å–
crawl("https://example.com")
```

## ğŸš¨ æ³¨æ„äº‹é¡¹

1. **å°Šé‡ç½‘ç«™è§„åˆ™**: éµå®ˆ robots.txt å’Œç½‘ç«™ä½¿ç”¨æ¡æ¬¾
2. **åˆç†é¢‘ç‡**: ä¸è¦è¿‡äºé¢‘ç¹åœ°è¯·æ±‚åŒä¸€ç½‘ç«™
3. **é”™è¯¯å¤„ç†**: ä½¿ç”¨ health_check å…ˆéªŒè¯ç½‘ç«™å¯ç”¨æ€§
4. **é€‰æ‹©å™¨å‡†ç¡®æ€§**: ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·ç¡®è®¤ CSS é€‰æ‹©å™¨
5. **å†…å®¹é•¿åº¦**: æ‰¹é‡çˆ¬å–æ—¶æ³¨æ„å†…å®¹æ€»é•¿åº¦

## ğŸ” æ•…éšœæ’é™¤

### é—®é¢˜ 1: çˆ¬å–å¤±è´¥
**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨ `health_check()` æ£€æŸ¥ç½‘ç«™å¯ç”¨æ€§
2. æ£€æŸ¥ç½‘ç»œè¿æ¥
3. ç¡®è®¤ URL æ ¼å¼æ­£ç¡®

### é—®é¢˜ 2: CSS é€‰æ‹©å™¨æ— æ•ˆ
**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·éªŒè¯é€‰æ‹©å™¨
2. å°è¯•æ›´é€šç”¨çš„é€‰æ‹©å™¨
3. æ£€æŸ¥é¡µé¢æ˜¯å¦ä¸ºåŠ¨æ€åŠ è½½å†…å®¹

### é—®é¢˜ 3: å†…å®¹ä¸ºç©º
**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥é€‰æ‹©å™¨æ˜¯å¦åŒ¹é…åˆ°å…ƒç´ 
2. å°è¯•ä¸ä½¿ç”¨é€‰æ‹©å™¨çš„åŸºç¡€çˆ¬å–
3. ç¡®è®¤é¡µé¢å†…å®¹ä¸æ˜¯ JavaScript åŠ¨æ€ç”Ÿæˆ

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ï¼š
1. æŸ¥çœ‹é”™è¯¯ä¿¡æ¯
2. è¿è¡Œ `python demo.py` æµ‹è¯•åŸºç¡€åŠŸèƒ½
3. æ£€æŸ¥ Crawl4AI æ–‡æ¡£: https://docs.crawl4ai.com/
4. æäº¤ Issue åˆ°é¡¹ç›®ä»“åº“
