#!/usr/bin/env python3
# server_v9.py - Context Scraper MCP Server V9
# Enhanced version with unified configuration management and user-configurable parameters

import sys
import os
from pathlib import Path

# ===== å·¥ä½œç›®å½•ä¿®æ­£ =====
# ç¡®ä¿æ— è®ºä»å“ªä¸ªç›®å½•å¯åŠ¨ï¼Œéƒ½èƒ½æ­£ç¡®æ‰¾åˆ°é¡¹ç›®èµ„æºæ–‡ä»¶
SCRIPT_DIR = Path(__file__).parent.absolute()
print(f"ğŸ”§ è„šæœ¬ç›®å½•: {SCRIPT_DIR}")
print(f"ğŸ”§ å½“å‰å·¥ä½œç›®å½•: {Path.cwd()}")

# å¦‚æœå½“å‰å·¥ä½œç›®å½•ä¸æ˜¯è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œåˆ™åˆ‡æ¢åˆ°è„šæœ¬ç›®å½•
if Path.cwd() != SCRIPT_DIR:
    print(f"ğŸ”„ åˆ‡æ¢å·¥ä½œç›®å½•: {Path.cwd()} -> {SCRIPT_DIR}")
    os.chdir(SCRIPT_DIR)
    print(f"âœ… å·¥ä½œç›®å½•å·²åˆ‡æ¢åˆ°: {Path.cwd()}")
else:
    print(f"âœ… å·¥ä½œç›®å½•æ­£ç¡®: {Path.cwd()}")
# ===== å·¥ä½œç›®å½•ä¿®æ­£ç»“æŸ =====

# ===== è‡ªåŠ¨æ¿€æ´»è™šæ‹Ÿç¯å¢ƒåŠŸèƒ½ =====
def activate_virtual_environment():
    """
    è‡ªåŠ¨æ¿€æ´»å½“å‰é¡¹ç›®çš„è™šæ‹Ÿç¯å¢ƒ
    è¿™ä¸ªå‡½æ•°ä¼šåœ¨å¯¼å…¥å…¶ä»–æ¨¡å—ä¹‹å‰æ‰§è¡Œï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„Pythonç¯å¢ƒ
    æ”¯æŒæ‰€æœ‰Pythonç‰ˆæœ¬ï¼ŒåŒ…æ‹¬æœªæ¥ç‰ˆæœ¬
    """
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_dir = Path(__file__).parent.absolute()
    venv_path = current_dir / ".venv"
    
    print(f"ğŸ” æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ: {venv_path}")
    
    if venv_path.exists():
        print(f"âœ… æ‰¾åˆ°è™šæ‹Ÿç¯å¢ƒç›®å½•: {venv_path}")
        
        # åŠ¨æ€æ£€æµ‹æ‰€æœ‰å¯ç”¨çš„Pythonç‰ˆæœ¬
        lib_path = venv_path / "lib"
        site_packages_path = None
        detected_version = None
        
        if lib_path.exists():
            print(f"ğŸ” æ‰«ælibç›®å½•: {lib_path}")
            
            # è·å–æ‰€æœ‰python*ç›®å½•ï¼Œè‡ªåŠ¨æ”¯æŒæœªæ¥ç‰ˆæœ¬
            python_dirs = []
            for item in lib_path.iterdir():
                if item.is_dir() and item.name.startswith('python'):
                    python_dirs.append(item.name)
            
            # æŒ‰ç‰ˆæœ¬å·æ’åºï¼Œä¼˜å…ˆä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
            python_dirs.sort(reverse=True)
            print(f"ğŸ” å‘ç°Pythonç‰ˆæœ¬: {python_dirs}")
            
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ…å«site-packagesçš„ç‰ˆæœ¬
            for py_version in python_dirs:
                potential_path = lib_path / py_version / "site-packages"
                print(f"ğŸ” æ£€æŸ¥è·¯å¾„: {potential_path}")
                if potential_path.exists():
                    site_packages_path = potential_path
                    detected_version = py_version
                    print(f"âœ… æ‰¾åˆ°å¯ç”¨çš„Pythonç‰ˆæœ¬: {py_version}")
                    break
        
        if site_packages_path:
            # å°†è™šæ‹Ÿç¯å¢ƒçš„ site-packages æ·»åŠ åˆ° sys.path çš„æœ€å‰é¢
            site_packages_str = str(site_packages_path)
            if site_packages_str not in sys.path:
                sys.path.insert(0, site_packages_str)
                print(f"âœ… è™šæ‹Ÿç¯å¢ƒå·²è‡ªåŠ¨æ¿€æ´»!")
                print(f"ğŸ“¦ Pythonç‰ˆæœ¬: {detected_version}")
                print(f"ğŸ“¦ Site-packagesè·¯å¾„: {site_packages_str}")
            else:
                print(f"â„¹ï¸  è™šæ‹Ÿç¯å¢ƒå·²åœ¨sys.pathä¸­")
                print(f"ğŸ“¦ å½“å‰Pythonç‰ˆæœ¬: {detected_version}")
            
            # è®¾ç½®è™šæ‹Ÿç¯å¢ƒç›¸å…³çš„ç¯å¢ƒå˜é‡
            os.environ['VIRTUAL_ENV'] = str(venv_path)
            
            # æ›´æ–°PATHç¯å¢ƒå˜é‡ï¼Œç¡®ä¿ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒçš„å¯æ‰§è¡Œæ–‡ä»¶
            venv_bin = venv_path / "bin"
            if venv_bin.exists():
                current_path = os.environ.get('PATH', '')
                if str(venv_bin) not in current_path:
                    os.environ['PATH'] = f"{venv_bin}:{current_path}"
                    print(f"ğŸ”§ PATHå·²æ›´æ–°ï¼Œä¼˜å…ˆä½¿ç”¨è™šæ‹Ÿç¯å¢ƒçš„å¯æ‰§è¡Œæ–‡ä»¶")
                else:
                    print(f"â„¹ï¸  è™šæ‹Ÿç¯å¢ƒbinç›®å½•å·²åœ¨PATHä¸­")
        else:
            print(f"âš ï¸  è™šæ‹Ÿç¯å¢ƒå­˜åœ¨ä½†æœªæ‰¾åˆ°site-packagesç›®å½•")
            if lib_path.exists():
                print(f"ğŸ“ libç›®å½•å†…å®¹:")
                for item in lib_path.iterdir():
                    print(f"   - {item.name} ({'ç›®å½•' if item.is_dir() else 'æ–‡ä»¶'})")
            else:
                print(f"ğŸ“ libç›®å½•ä¸å­˜åœ¨: {lib_path}")
    else:
        print(f"âš ï¸  è™šæ‹Ÿç¯å¢ƒç›®å½•ä¸å­˜åœ¨: {venv_path}")
        print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿å·²åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (.venv)")
        print("ğŸ’¡ åˆ›å»ºå‘½ä»¤: uv sync æˆ– python -m venv .venv")

# åœ¨å¯¼å…¥ä»»ä½•å…¶ä»–æ¨¡å—ä¹‹å‰æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
print("ğŸš€ æ­£åœ¨å¯åŠ¨ Context Scraper MCP Server V9...")
activate_virtual_environment()

# ===== å¯¼å…¥ä¾èµ–æ¨¡å— =====

import asyncio
import json
import time
from typing import Optional, List, Dict, Any

# V9 core components
from v9_core.intent_analyzer import analyze_user_intent, UserIntent, SearchEngineIntent, IntentType
from v9_core.crawl_config_manager import get_crawl_config, reload_crawl_config

# Crawl4AI components
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from mcp.server.fastmcp import Context, FastMCP

# åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
config = get_crawl_config()
print(f"âš™ï¸  é…ç½®ç®¡ç†å™¨å·²åˆå§‹åŒ–")

# Create MCP server
mcp = FastMCP("ContextScraperV9")

# ===== é…ç½®ç®¡ç†å·¥å…· =====

@mcp.tool()
async def configure_crawl_settings(
    action: str = "show",
    setting_type: str = "all",
    **kwargs
) -> str:
    """
    é…ç½®çˆ¬å–å‚æ•°è®¾ç½®
    
    Args:
        action: æ“ä½œç±»å‹ (show/update/reset)
        setting_type: è®¾ç½®ç±»å‹ (content_limits/quality_control/timing_control/user_preferences/all)
        **kwargs: å…·ä½“çš„é…ç½®å‚æ•°
        
    Returns:
        é…ç½®æ“ä½œç»“æœ
        
    Use cases:
        - æŸ¥çœ‹å½“å‰é…ç½®: configure_crawl_settings("show", "all")
        - æ›´æ–°å†…å®¹é™åˆ¶: configure_crawl_settings("update", "content_limits", markdown_display_limit=5000)
        - æ›´æ–°ç”¨æˆ·åå¥½: configure_crawl_settings("update", "user_preferences", show_word_count=False)
    """
    
    try:
        global config
        
        if action == "show":
            if setting_type == "all":
                return config.get_config_summary()
            elif setting_type == "content_limits":
                return f"""ğŸ“„ å†…å®¹é™åˆ¶é…ç½®:
- Markdownæ˜¾ç¤ºé™åˆ¶: {config.content_limits.markdown_display_limit} å­—ç¬¦
- Claudeé¢„è§ˆé™åˆ¶: {config.content_limits.claude_preview_limit} å­—ç¬¦
- åŸºç¡€çˆ¬å–æ— é™åˆ¶: {config.content_limits.basic_crawl_unlimited}"""
            elif setting_type == "quality_control":
                return f"""ğŸ¯ è´¨é‡æ§åˆ¶é…ç½®:
- è¯æ•°é˜ˆå€¼: {config.quality_control.word_count_threshold} è¯
- æœ€å°è´¨é‡: {config.quality_control.min_content_quality}"""
            elif setting_type == "timing_control":
                return f"""â±ï¸ æ—¶é—´æ§åˆ¶é…ç½®:
- é¡µé¢è¶…æ—¶: {config.timing_control.page_timeout_ms}ms
- éšèº«å»¶è¿Ÿ: {config.timing_control.stealth_delay_seconds}s
- åŠ¨æ€å†…å®¹å»¶è¿Ÿ: {config.timing_control.dynamic_content_delay_seconds}s"""
            elif setting_type == "user_preferences":
                return f"""ğŸ‘¤ ç”¨æˆ·åå¥½è®¾ç½®:
- è¯¦ç»†æ—¥å¿—: {config.user_preferences.show_detailed_logs}
- æ˜¾ç¤ºè¯æ•°: {config.user_preferences.show_word_count}
- æ˜¾ç¤ºæ—¶é—´: {config.user_preferences.show_timing_info}
- ç´§å‡‘è¾“å‡º: {config.user_preferences.compact_output}"""
        
        elif action == "update":
            if setting_type == "content_limits":
                config.update_content_limits(**kwargs)
                return f"âœ… å†…å®¹é™åˆ¶é…ç½®å·²æ›´æ–°: {kwargs}"
            elif setting_type == "quality_control":
                config.update_quality_control(**kwargs)
                return f"âœ… è´¨é‡æ§åˆ¶é…ç½®å·²æ›´æ–°: {kwargs}"
            elif setting_type == "timing_control":
                config.update_timing_control(**kwargs)
                return f"âœ… æ—¶é—´æ§åˆ¶é…ç½®å·²æ›´æ–°: {kwargs}"
            elif setting_type == "user_preferences":
                config.update_user_preferences(**kwargs)
                return f"âœ… ç”¨æˆ·åå¥½è®¾ç½®å·²æ›´æ–°: {kwargs}"
        
        elif action == "reset":
            config = reload_crawl_config()
            return "âœ… é…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼"
        
        return f"âŒ ä¸æ”¯æŒçš„æ“ä½œ: action={action}, setting_type={setting_type}"
        
    except Exception as e:
        return f"âŒ é…ç½®æ“ä½œå¤±è´¥: {str(e)}"

@mcp.tool()
async def quick_config_content_limit(limit: int = 3000) -> str:
    """
    å¿«é€Ÿè®¾ç½®å†…å®¹æ˜¾ç¤ºé™åˆ¶
    
    Args:
        limit: å†…å®¹æ˜¾ç¤ºå­—ç¬¦æ•°é™åˆ¶
        
    Returns:
        è®¾ç½®ç»“æœ
        
    Use cases:
        - è®¾ç½®5000å­—ç¬¦é™åˆ¶: quick_config_content_limit(5000)
        - è®¾ç½®1000å­—ç¬¦é™åˆ¶: quick_config_content_limit(1000)
    """
    try:
        global config
        config.update_content_limits(markdown_display_limit=limit)
        return f"âœ… å†…å®¹æ˜¾ç¤ºé™åˆ¶å·²è®¾ç½®ä¸º: {limit} å­—ç¬¦"
    except Exception as e:
        return f"âŒ è®¾ç½®å¤±è´¥: {str(e)}"

@mcp.tool()
async def quick_config_word_threshold(threshold: int = 50) -> str:
    """
    å¿«é€Ÿè®¾ç½®è¯æ•°é˜ˆå€¼
    
    Args:
        threshold: æœ€å°è¯æ•°é˜ˆå€¼
        
    Returns:
        è®¾ç½®ç»“æœ
        
    Use cases:
        - è®¾ç½®100è¯é˜ˆå€¼: quick_config_word_threshold(100)
        - è®¾ç½®20è¯é˜ˆå€¼: quick_config_word_threshold(20)
    """
    try:
        global config
        config.update_quality_control(word_count_threshold=threshold)
        return f"âœ… è¯æ•°é˜ˆå€¼å·²è®¾ç½®ä¸º: {threshold} è¯"
    except Exception as e:
        return f"âŒ è®¾ç½®å¤±è´¥: {str(e)}"

# ===== è¾…åŠ©å‡½æ•° =====

def format_crawl_result(result, url: str, tool_name: str, extra_info: Dict[str, Any] = None) -> str:
    """
    æ ¼å¼åŒ–çˆ¬å–ç»“æœï¼Œä½¿ç”¨é…ç½®ç®¡ç†å™¨çš„è®¾ç½®
    
    Args:
        result: çˆ¬å–ç»“æœå¯¹è±¡
        url: ç›®æ ‡URL
        tool_name: å·¥å…·åç§°
        extra_info: é¢å¤–ä¿¡æ¯å­—å…¸
        
    Returns:
        æ ¼å¼åŒ–çš„ç»“æœå­—ç¬¦ä¸²
    """
    global config
    
    if not result.success:
        return f"{tool_name} å¤±è´¥\n\nURL: {url}\nError: {result.error_message}"
    
    # åŸºç¡€ä¿¡æ¯
    response = f"{tool_name} æˆåŠŸ\n\nURL: {url}\n"
    
    # æ ‡é¢˜ä¿¡æ¯
    title = result.metadata.get('title', 'Unknown')
    response += f"Title: {title}\n"
    
    # è¯æ•°ç»Ÿè®¡ï¼ˆæ ¹æ®ç”¨æˆ·åå¥½ï¼‰
    if config.user_preferences.show_word_count:
        word_count = len(result.markdown.split()) if result.markdown else 0
        response += f"Word Count: {word_count}\n"
    
    # é¢å¤–ä¿¡æ¯
    if extra_info:
        for key, value in extra_info.items():
            response += f"{key}: {value}\n"
    
    # å†…å®¹æ˜¾ç¤º
    if result.markdown:
        if config.content_limits.basic_crawl_unlimited and tool_name == "Basic Crawl":
            # åŸºç¡€çˆ¬å–æ˜¾ç¤ºå®Œæ•´å†…å®¹
            response += f"\nContent:\n\n{result.markdown}"
        else:
            # å…¶ä»–å·¥å…·ä½¿ç”¨é…ç½®çš„é™åˆ¶
            limit = config.content_limits.markdown_display_limit
            if len(result.markdown) > limit:
                response += f"\nContent (å‰{limit}å­—ç¬¦):\n\n{result.markdown[:limit]}..."
            else:
                response += f"\nContent:\n\n{result.markdown}"
    else:
        response += "\nContent: æ— å†…å®¹"
    
    return response

def get_crawler_config(tool_type: str = "default") -> CrawlerRunConfig:
    """
    æ ¹æ®å·¥å…·ç±»å‹è·å–çˆ¬å–é…ç½®
    
    Args:
        tool_type: å·¥å…·ç±»å‹ (default/stealth/geolocation/retry/intelligence)
        
    Returns:
        é…ç½®å¥½çš„CrawlerRunConfigå¯¹è±¡
    """
    global config
    
    # åŸºç¡€é…ç½®
    cache_mode = CacheMode.BYPASS if config.cache_control.default_cache_mode == "BYPASS" else CacheMode.ENABLED
    
    base_config = {
        "cache_mode": cache_mode,
        "word_count_threshold": config.quality_control.word_count_threshold,
    }
    
    # æ ¹æ®å·¥å…·ç±»å‹æ·»åŠ ç‰¹å®šé…ç½®
    if tool_type == "default":
        base_config.update({
            "page_timeout": config.timing_control.page_timeout_ms,
            "wait_until": config.browser_control.default_wait_until
        })
    elif tool_type == "stealth":
        base_config.update({
            "delay_before_return_html": config.timing_control.stealth_delay_seconds
        })
    elif tool_type == "intelligence":
        # æ™ºèƒ½æ¨¡å¼æ ¹æ®å†…å®¹ç±»å‹åŠ¨æ€è®¾ç½®å»¶è¿Ÿ
        base_config.update({
            "delay_before_return_html": config.timing_control.dynamic_content_delay_seconds
        })
    
    return CrawlerRunConfig(**base_config)

# ===== V6 Core Features =====

@mcp.prompt()
def smart_search_guide(search_query: str = "What do you want to search?") -> str:
    """Smart Search Guide - AI-driven search tool selection assistant
    
    Intelligently recommends the most suitable crawling tools based on search content,
    provides decision trees and specific execution suggestions.
    """
    
    # URL encoding
    import urllib.parse
    encoded_query = urllib.parse.quote_plus(search_query)
    
    # Build search engine URLs
    google_url = f"https://www.google.com/search?q={encoded_query}"
    baidu_url = f"https://www.baidu.com/s?wd={encoded_query}"
    bing_url = f"https://www.bing.com/search?q={encoded_query}"
    duckduckgo_url = f"https://duckduckgo.com/?q={encoded_query}"
    
    # Smart analysis of search content
    query_lower = search_query.lower()
    
    # Detect language and content type
    has_chinese = any('\u4e00' <= char <= '\u9fff' for char in search_query)
    
    # Technical keywords (expanded)
    technical_keywords = [
        'api', 'code', 'programming', 'github', 'stackoverflow', 'python', 'javascript', 'java', 'react', 'vue',
        'docker', 'kubernetes', 'aws', 'cloud', 'database', 'sql', 'nosql', 'mongodb', 'redis', 'nginx',
        'linux', 'ubuntu', 'centos', 'bash', 'shell', 'git', 'devops', 'ci/cd', 'jenkins', 'terraform',
        'machine learning', 'ai', 'deep learning', 'tensorflow', 'pytorch', 'data science', 'algorithm',
        'framework', 'library', 'sdk', 'compiler', 'debugger', 'ide', 'vscode', 'intellij',
        'ç¼–ç¨‹', 'ä»£ç ', 'å¼€å‘', 'æŠ€æœ¯', 'ç®—æ³•', 'æ•°æ®åº“', 'æœåŠ¡å™¨', 'äº‘è®¡ç®—', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ '
    ]
    
    # Academic keywords (expanded)
    academic_keywords = [
        'research', 'paper', 'study', 'journal', 'publication', 'thesis', 'dissertation', 'conference',
        'academic', 'scholar', 'university', 'college', 'professor', 'phd', 'master', 'bachelor',
        'citation', 'bibliography', 'peer review', 'methodology', 'analysis', 'experiment', 'survey',
        'arxiv', 'pubmed', 'ieee', 'acm', 'springer', 'elsevier', 'nature', 'science',
        'ç ”ç©¶', 'è®ºæ–‡', 'å­¦æœ¯', 'æœŸåˆŠ', 'ä¼šè®®', 'å¤§å­¦', 'å­¦è€…', 'åšå£«', 'ç¡•å£«', 'å®éªŒ', 'è°ƒç ”'
    ]
    
    # News keywords (expanded)
    news_keywords = [
        'news', 'latest', 'breaking', 'update', 'report', 'announcement', 'press release', 'headline',
        'current', 'today', 'yesterday', 'recent', 'happening', 'event', 'incident', 'story',
        'æ–°é—»', 'æœ€æ–°', 'ä»Šæ—¥', 'æ˜¨æ—¥', 'æœ€è¿‘', 'äº‹ä»¶', 'æŠ¥é“', 'æ¶ˆæ¯', 'å¤´æ¡', 'å¿«è®¯'
    ]
    
    # Privacy keywords (expanded)
    privacy_keywords = [
        'privacy', 'anonymous', 'private', 'secure', 'confidential', 'hidden', 'secret', 'vpn',
        'tor', 'encryption', 'security', 'protect', 'safe', 'incognito', 'stealth',
        'éšç§', 'åŒ¿å', 'ç§å¯†', 'å®‰å…¨', 'ä¿æŠ¤', 'åŠ å¯†', 'ç§˜å¯†', 'éšèº«'
    ]
    
    is_technical = any(keyword in query_lower for keyword in technical_keywords)
    is_academic = any(keyword in query_lower for keyword in academic_keywords)
    is_news = any(keyword in query_lower for keyword in news_keywords)
    is_sensitive = any(keyword in query_lower for keyword in privacy_keywords)
    
    # Smart recommendation with priority logic
    if is_academic:
        # Academic content gets specialized academic search
        primary_recommendation = "Academic Search"
        primary_command = f'academic_search("{search_query}", "google_scholar", 5)'
        primary_reason = "Academic content detected, specialized academic search recommended for high-quality research results"
    elif is_sensitive:
        # Privacy is highest priority
        primary_recommendation = "DuckDuckGo Search"
        primary_command = f'crawl_with_intelligence("{duckduckgo_url}", "smart")'
        primary_reason = "Privacy-sensitive content detected, DuckDuckGo doesn't track users"
    elif is_technical:
        # Technical content gets Google with deep search
        primary_recommendation = "Google Deep Search"
        primary_command = f'crawl_with_intelligence("{google_url}", "deep", 5)'
        primary_reason = "Technical content detected, Google has the richest technical resources, deep search for comprehensive results"
    else:
        # Default to Google for general English content
        primary_recommendation = "Google Search"
        primary_command = f'crawl_with_intelligence("{google_url}", "smart")'
        primary_reason = "General search, Google has the widest coverage and best algorithms"
    
    # Build feature analysis display
    features = []
    if has_chinese:
        features.append("Chinese")
    else:
        features.append("English")
    
    if is_technical:
        features.append("Technical")
    if is_academic:
        features.append("Academic")
    if is_news:
        features.append("News")
    if is_sensitive:
        features.append("Privacy-Sensitive")
    
    if len(features) == 1:  # Only language detected
        features.append("General")
    
    features_display = " | ".join(features)
    
    # è·å–å½“å‰é…ç½®ä¿¡æ¯
    global config
    content_limit = config.content_limits.markdown_display_limit
    
    return f"""# ğŸ” AI Smart Search Assistant (V9 å­¦æœ¯å¢å¼ºç‰ˆ)

## ğŸ“Š Search Analysis
**Query**: {search_query}
**Content Features**: {features_display}
**Search Type**: {'ğŸ“ Academic' if is_academic else 'ğŸŒ General'}
**Confidence**: {'High' if len([f for f in [is_technical, is_academic, is_news, is_sensitive] if f]) > 0 else 'Medium'}
**Current Content Limit**: {content_limit} å­—ç¬¦

## ğŸ¯ AI Recommended Solution (Use First)

### â­ Recommended: {primary_recommendation}
**Analysis**: {primary_reason}

**ğŸš€ Execute Now**:
```
{primary_command}
```

## ğŸ”§ Search Options Matrix

### ğŸ“ Academic & Research
| Use Case | Tool | Command Example |
|----------|------|-----------------|
| å­¦æœ¯è®ºæ–‡æœç´¢ | Academic Search | `academic_search("machine learning", "google_scholar", 5)` |
| arXivè®ºæ–‡ | Academic Search | `academic_search("deep learning", "arxiv", 3)` |
| åŒ»å­¦æ–‡çŒ® | Academic Search | `academic_search("COVID-19", "pubmed", 5)` |

### ğŸŒ General Web Search  
| Use Case | Tool | Command Example |
|----------|------|-----------------|
| æ·±åº¦æœç´¢ | Intelligence + Deep | `crawl_with_intelligence("{google_url}", "deep", 5)` |
| æ™ºèƒ½æœç´¢ | Intelligence + Smart | `crawl_with_intelligence("{google_url}", "smart")` |
| åŸºç¡€æœç´¢ | Intelligence + Basic | `crawl_with_intelligence("{google_url}", "basic")` |
| éšç§æœç´¢ | Intelligence | `crawl_with_intelligence("{duckduckgo_url}", "smart")` |
| ä¸­æ–‡åœ°åŸŸæœç´¢ | Intelligence | `crawl_with_intelligence("{baidu_url}", "smart")` |

### ğŸ›¡ï¸ Anti-Detection Options
| Tool | Use Case | Command |
|------|----------|---------|
| Stealth Mode | Basic anti-bot | `crawl_stealth("{google_url}")` |
| Geo Spoofing | Regional restrictions | `crawl_with_geolocation("{google_url}", "random")` |
| Retry Mode | Unstable sites | `crawl_with_retry("{google_url}")` |

## ğŸ¤” Decision Tree

```
æœç´¢éœ€æ±‚åˆ†æ
    â†“
å­¦æœ¯ç ”ç©¶? â†’ Yes â†’ academic_search() âœ…
    â†“ No
éœ€è¦æ·±åº¦å†…å®¹? â†’ Yes â†’ crawl_with_intelligence(url, "deep", 5) âœ…
    â†“ No  
éšç§æ•æ„Ÿ? â†’ Yes â†’ DuckDuckGo + crawl_with_intelligence(url, "smart") âœ…
    â†“ No
æŠ€æœ¯å†…å®¹? â†’ Yes â†’ Google + crawl_with_intelligence(url, "deep", 3) âœ…
    â†“ No
ä¸€èˆ¬æœç´¢ â†’ Google + crawl_with_intelligence(url, "smart") âœ…
```

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### ğŸ“ å­¦æœ¯æœç´¢åœºæ™¯
- **è®ºæ–‡æŸ¥æ‰¾**: ä½¿ç”¨ `academic_search()` è·å–é«˜è´¨é‡å­¦æœ¯å†…å®¹
- **æ–‡çŒ®ç»¼è¿°**: è®¾ç½®è¾ƒå¤§çš„ `deep_crawl_count` å‚æ•° (5-10)
- **ç‰¹å®šé¢†åŸŸ**: é€‰æ‹©åˆé€‚çš„ `source` (google_scholar/arxiv/pubmed)

### ğŸŒ ä¸€èˆ¬æœç´¢åœºæ™¯  
- **ä¿¡æ¯ä¸°å¯Œåº¦ä¼˜å…ˆ**: ä½¿ç”¨ `crawl_mode="deep"` è·å–å®Œæ•´å†…å®¹
- **é€Ÿåº¦ä¼˜å…ˆ**: ä½¿ç”¨ `crawl_mode="smart"` å¿«é€Ÿè·å–ä¼˜åŒ–å†…å®¹
- **åŸºç¡€éœ€æ±‚**: ä½¿ç”¨ `crawl_mode="basic"` è·å–åŸå§‹å†…å®¹
- **éšç§ä¿æŠ¤**: é€‰æ‹©DuckDuckGoæœç´¢å¼•æ“

### âš™ï¸ é…ç½®ä¼˜åŒ–
- å†…å®¹é•¿åº¦: `quick_config_content_limit(5000)`
- è¯æ•°é˜ˆå€¼: `quick_config_word_threshold(100)`
- æŸ¥çœ‹é…ç½®: `configure_crawl_settings("show", "all")`

## ğŸ¯ Quick Start Examples

```python
# ğŸ“ å­¦æœ¯æœç´¢
academic_search("transformer architecture", "arxiv", 3)

# ğŸŒ æ·±åº¦ç½‘é¡µæœç´¢  
crawl_with_intelligence("https://google.com/search?q=AI+news", "deep", 5)

# ğŸ” æ™ºèƒ½ä¿¡æ¯è·å–
crawl_with_intelligence("https://google.com/search?q=weather", "smart")

# ğŸ“„ åŸºç¡€é¡µé¢çˆ¬å–
crawl_with_intelligence("https://example.com", "basic")
```

## âš ï¸ Important Notes

- æ·±åº¦æœç´¢ä¼šçˆ¬å–æœç´¢ç»“æœä¸­çš„å®é™…ç½‘é¡µå†…å®¹ï¼Œè€—æ—¶è¾ƒé•¿ä½†ä¿¡æ¯æ›´ä¸°å¯Œ
- å­¦æœ¯æœç´¢ä¸“é—¨ä¼˜åŒ–äº†å¯¹å­¦æœ¯ç½‘ç«™çš„è§£æ
- ä½¿ç”¨åˆç†çš„å»¶è¿Ÿé¿å…è¢«ç½‘ç«™å±è”½
- å½“å‰è®¾ç½®å¯é€šè¿‡ `configure_crawl_settings("show")` æŸ¥çœ‹

**ğŸš€ Ready to search? Start with the AI recommended solution above!**
"""
# ===== æ·±åº¦æœç´¢è¾…åŠ©å‡½æ•° =====

def _is_search_page(url: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæœç´¢é¡µé¢"""
    search_indicators = [
        'google.com/search',
        'baidu.com/s',
        'bing.com/search', 
        'duckduckgo.com',
        'scholar.google.com',
        'arxiv.org/search',
        'pubmed.ncbi.nlm.nih.gov'
    ]
    return any(indicator in url.lower() for indicator in search_indicators)

def _extract_search_result_links(markdown_content: str, deep_crawl_count: int) -> List[str]:
    """ä»æœç´¢ç»“æœé¡µé¢æå–é“¾æ¥"""
    import re
    
    # æå–markdownä¸­çš„é“¾æ¥
    link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
    matches = re.findall(link_pattern, markdown_content)
    
    # è¿‡æ»¤æ‰æœç´¢å¼•æ“è‡ªèº«çš„é“¾æ¥å’Œæ— æ•ˆé“¾æ¥
    filtered_links = []
    exclude_patterns = [
        'google.com', 'baidu.com', 'bing.com', 'duckduckgo.com',
        'javascript:', 'mailto:', '#', 'webcache', 'translate.google'
    ]
    
    for title, link in matches:
        # è·³è¿‡æ’é™¤çš„é“¾æ¥æ¨¡å¼
        if any(pattern in link.lower() for pattern in exclude_patterns):
            continue
            
        # ç¡®ä¿æ˜¯æœ‰æ•ˆçš„HTTPé“¾æ¥
        if link.startswith(('http://', 'https://')):
            filtered_links.append(link)
            
        if len(filtered_links) >= deep_crawl_count:
            break
    
    return filtered_links

async def _crawl_search_results(crawler, links: List[str], crawl_config) -> str:
    """çˆ¬å–æœç´¢ç»“æœé“¾æ¥çš„å†…å®¹"""
    results = []
    
    for i, link in enumerate(links, 1):
        try:
            print(f"ğŸ” æ­£åœ¨çˆ¬å–ç¬¬{i}ä¸ªæœç´¢ç»“æœ: {link}")
            
            result = await crawler.arun(url=link, config=crawl_config)
            
            if result.success and result.markdown:
                # é™åˆ¶æ¯ä¸ªç»“æœçš„é•¿åº¦ï¼Œé¿å…å†…å®¹è¿‡é•¿
                content = result.markdown[:2000] if len(result.markdown) > 2000 else result.markdown
                title = result.metadata.get('title', f'æœç´¢ç»“æœ {i}')
                
                results.append(f"## ğŸ“„ {title}\n**URL**: {link}\n\n{content}\n")
            else:
                results.append(f"## âŒ æœç´¢ç»“æœ {i}\n**URL**: {link}\n**é”™è¯¯**: æ— æ³•è·å–å†…å®¹\n")
                
        except Exception as e:
            results.append(f"## âŒ æœç´¢ç»“æœ {i}\n**URL**: {link}\n**é”™è¯¯**: {str(e)}\n")
    
    return "\n".join(results) if results else "æœªèƒ½è·å–åˆ°æœ‰æ•ˆçš„æœç´¢ç»“æœå†…å®¹"

# ===== é…ç½®åŒ–çˆ¬å–å·¥å…· =====

@mcp.tool()
async def crawl(url: str) -> str:
    """
    Basic webpage crawling with Markdown conversion (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Args:
        url: Target webpage URL
        
    Returns:
        Webpage content in Markdown format
        
    Use cases:
        - Simple content extraction
        - Static webpage crawling  
        - Quick content preview
    """
    try:
        global config
        
        browser_config = BrowserConfig(
            headless=config.browser_control.headless_mode,
            browser_type=config.browser_control.browser_type
        )
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            crawl_config = get_crawler_config("default")
            result = await crawler.arun(url=url, config=crawl_config)
            
            return format_crawl_result(result, url, "Basic Crawl")
                
    except Exception as e:
        return f"Basic Crawl Error\n\nURL: {url}\nException: {str(e)}"

@mcp.tool()
async def crawl_stealth(url: str) -> str:
    """
    Stealth web crawling with anti-detection techniques (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Args:
        url: Target webpage URL
        
    Returns:
        Webpage content crawled with stealth techniques
        
    Use cases:
        - Bypass anti-bot protection
        - Avoid rate limiting detection
        - Privacy-focused crawling
    """
    
    try:
        global config
        
        # Import anti-detection functionality
        import sys
        sys.path.append('legacy/servers')
        from anti_detection import create_stealth_config
        
        # Use stealth configuration
        browser_config = create_stealth_config()
        crawl_config = get_crawler_config("stealth")
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=crawl_config) 
            
            if result.success:
                # Show disguise information
                ua_info = browser_config.user_agent[:80] + "..." if len(browser_config.user_agent) > 80 else browser_config.user_agent
                viewport_info = f"{browser_config.viewport_width}x{browser_config.viewport_height}"
                
                extra_info = {
                    "Disguised UA": ua_info,
                    "Viewport": viewport_info,
                    "Anti-Detection": "Enabled",
                    "Stealth Delay": f"{config.timing_control.stealth_delay_seconds}s"
                }
                
                return format_crawl_result(result, url, "Stealth Crawling", extra_info)
            else:
                return f"Stealth crawling failed: {result.error_message}"
                
    except ImportError:
        return "Anti-detection module not found, please check legacy/servers/anti_detection.py"
    except Exception as e:
        return f"Stealth crawling error: {str(e)}"

@mcp.tool()
async def crawl_with_geolocation(url: str, location: str = "random") -> str:
    """
    Geolocation spoofing crawl to bypass regional restrictions (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Args:
        url: Target webpage URL
        location: Geographic location (random/newyork/london/tokyo/paris/berlin/toronto/singapore/sydney)
        
    Returns:
        Webpage content with geolocation spoofing information
        
    Use cases:
        - Access region-locked content
        - Test location-based features
        - Bypass geographic restrictions
    """
    
    try:
        global config
        
        # Import geolocation spoofing functionality
        import sys
        sys.path.append('legacy/servers')
        from anti_detection import create_geo_spoofed_config
        
        # Create geolocation spoofing configuration
        browser_config, geo_config = create_geo_spoofed_config(location)
        crawl_config = get_crawler_config("geolocation")
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=crawl_config)
            
            if result.success:
                extra_info = {
                    "Spoofed Location": f"Lat {geo_config.latitude:.4f}, Lng {geo_config.longitude:.4f}",
                    "Accuracy": f"{geo_config.accuracy:.1f}m"
                }
                
                return format_crawl_result(result, url, "Geolocation Spoofing Crawl", extra_info)
            else:
                return f"Geolocation spoofing crawl failed: {result.error_message}"
                
    except ImportError:
        return "Geolocation spoofing module not found, please check legacy/servers/anti_detection.py"
    except Exception as e:
        return f"Geolocation spoofing crawl error: {str(e)}"

@mcp.tool()
async def crawl_with_retry(url: str, max_retries: Optional[int] = None) -> str:
    """
    Retry crawling with exponential backoff for unstable websites (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Args:
        url: Target webpage URL
        max_retries: Maximum number of retry attempts (å¦‚æœä¸æŒ‡å®šï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼)
        
    Returns:
        Webpage content with retry attempt information
        
    Use cases:
        - Handle unstable network connections
        - Retry temporarily unavailable websites
        - Overcome intermittent failures
    """
    
    try:
        global config
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é‡è¯•æ¬¡æ•°ï¼Œé™¤éç”¨æˆ·æ˜ç¡®æŒ‡å®š
        if max_retries is None:
            max_retries = config.retry_control.max_retries
        
        # Import retry manager
        import sys
        sys.path.append('legacy/servers')
        from anti_detection import create_retry_manager, create_stealth_config
        
        retry_manager = create_retry_manager(max_retries)
        browser_config = create_stealth_config()  # Use stealth mode to improve success rate
        crawl_config = get_crawler_config("retry")
        
        start_time = time.time()
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await retry_manager.execute_with_retry(crawler, url, crawl_config)
            
            elapsed_time = time.time() - start_time
            
            if result.success:
                extra_info = {
                    "Time Taken": f"{elapsed_time:.2f}s",
                    "Max Retries": str(max_retries),
                    "Stealth Mode": "Enabled"
                }
                
                return format_crawl_result(result, url, "Retry Crawling", extra_info)
            else:
                return f"Retry crawling failed: {result.error_message}"
                
    except ImportError:
        return "Retry management module not found, please check legacy/servers/anti_detection.py"
    except Exception as e:
        return f"Retry crawling error: {str(e)}"

@mcp.tool()
async def crawl_with_intelligence(
    url: str,
    crawl_mode: str = "smart",
    deep_crawl_count: int = 3
) -> str:
    """
    Smart web crawling with content optimization and analysis (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Args:
        url: Target webpage URL
        crawl_mode: Crawling mode ("basic" | "smart" | "deep")
            - "basic": Basic crawling, just get the page content
            - "smart": Smart analysis and content optimization (default)
            - "deep": Deep crawling, extract and crawl search result links
        deep_crawl_count: Number of search result links to crawl when crawl_mode="deep" (1-10)
        
    Returns:
        Optimized webpage content in Markdown format
        
    Use cases:
        - Basic crawling: crawl_with_intelligence("https://example.com", "basic")
        - Smart crawling: crawl_with_intelligence("https://example.com", "smart") 
        - Deep search: crawl_with_intelligence("https://google.com/search?q=AI", "deep", 5)
    """
    
    try:
        global config
        
        # æ ¹æ®çˆ¬å–æ¨¡å¼è®¾ç½®å‚æ•°
        use_smart_analysis = crawl_mode in ["smart", "deep"]
        deep_search = crawl_mode == "deep"
        
        # Analyze URL intent
        if use_smart_analysis and config.advanced_settings.enable_smart_analysis:
            intent = analyze_user_intent(f"crawl {url}")
            
            # Adjust crawling strategy based on intent
            browser_config = BrowserConfig(
                headless=config.browser_control.headless_mode,
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            )
            
            # åŠ¨æ€è°ƒæ•´å»¶è¿Ÿæ—¶é—´
            delay_time = config.timing_control.dynamic_content_delay_seconds if intent.dynamic_content else config.timing_control.default_delay_seconds
            
            crawl_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS if config.cache_control.default_cache_mode == "BYPASS" else CacheMode.ENABLED,
                word_count_threshold=config.quality_control.word_count_threshold,
                delay_before_return_html=delay_time
            )
        else:
            browser_config = BrowserConfig(headless=config.browser_control.headless_mode)
            crawl_config = get_crawler_config("intelligence")
        
        # Execute crawling
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=crawl_config)
            
            if result.success:
                extra_info = {}
                extra_info["Crawl Mode"] = crawl_mode.title()
                
                # æ·±åº¦æœç´¢åŠŸèƒ½
                if deep_search and _is_search_page(url):
                    extra_info["Deep Crawl Count"] = str(deep_crawl_count)
                    
                    # è§£ææœç´¢ç»“æœé¡µé¢ï¼Œæå–é“¾æ¥
                    search_links = _extract_search_result_links(result.markdown, deep_crawl_count)
                    
                    if search_links:
                        deep_content = await _crawl_search_results(crawler, search_links, crawl_config)
                        
                        # ä»é…ç½®ä¸­è·å–åˆ†éš”ç¬¦é•¿åº¦ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼50
                        separator_length = getattr(config.user_preferences, 'separator_length', 50)
                        separator = '=' * separator_length
                        
                        # åˆå¹¶åŸå§‹æœç´¢é¡µé¢å’Œæ·±åº¦å†…å®¹
                        combined_content = f"{result.markdown}\n\n{separator}\nğŸ” DEEP SEARCH RESULTS\n{separator}\n\n{deep_content}"
                        
                        # åˆ›å»ºæ–°çš„ç»“æœå¯¹è±¡
                        class DeepResult:
                            def __init__(self, original_result, combined_content):
                                self.success = original_result.success
                                self.markdown = combined_content
                                self.metadata = original_result.metadata
                        
                        result = DeepResult(result, combined_content)
                
                return format_crawl_result(result, url, "V9 Smart Crawling", extra_info)
            else:
                return f"Crawling failed: {result.error_message}"
                
    except Exception as e:
        return f"Crawling process error: {str(e)}"
@mcp.tool()
async def academic_search(
    query: str,
    source: str = "google_scholar",
    deep_crawl_count: int = 5,
    num_search_results: int = 50,
    include_abstracts: bool = True
) -> str:
    """
    Academic search with paper content extraction using optimized search methods.
    
    Args:
        query: Academic search query
        source: Academic source (google_scholar/arxiv/pubmed)
        deep_crawl_count: Number of paper links to crawl in detail when using deep mode (1-10)
        num_search_results: Number of search results to request (default 50, only for Google Scholar)
        include_abstracts: Whether to include paper abstracts (currently for display info only)
        
    Returns:
        Academic search results with paper details
        
    Use cases:
        - Basic search: academic_search("machine learning", "google_scholar")
        - More results: academic_search("transformer", "google_scholar", 3, 100) 
        - arXiv papers: academic_search("deep learning", "arxiv")
        - Medical literature: academic_search("COVID-19", "pubmed")
        
    Note:
        - Google Scholar uses site:scholar.google.com method to bypass restrictions
        - Uses stealth mode by default for better success rate against anti-bot protection
        - deep_crawl_count is for future deep crawling feature
        - num_search_results only applies to Google Scholar search
    """
    
    try:
        global config
        
        # URL encoding
        import urllib.parse
        encoded_query = urllib.parse.quote_plus(query)
        
        # ğŸ†• ä½¿ç”¨ä¼˜åŒ–çš„æœç´¢æ–¹æ³•
        if source == "google_scholar":
            # ä½¿ç”¨ site:scholar.google.com è¯­æ³•é€šè¿‡æ™®é€š Google æœç´¢
            search_url = f"https://www.google.com/search?q=site:scholar.google.com+\"{query}\"&num={num_search_results}"
            crawl_method = "Google Site Search (Optimized)"
            search_info = f"æœç´¢ç»“æœæ•°: {num_search_results}"
        elif source == "arxiv":
            # arXiv ç›´æ¥æœç´¢ï¼Œä½¿ç”¨é»˜è®¤è¿”å›æ•°é‡
            search_url = f"https://arxiv.org/search/?query={encoded_query}&searchtype=all"
            crawl_method = "arXiv Direct Search"
            search_info = "æœç´¢ç»“æœæ•°: é»˜è®¤ (é€šå¸¸50æ¡)"
        elif source == "pubmed":
            # PubMed æœç´¢ï¼Œä½¿ç”¨é»˜è®¤è¿”å›æ•°é‡
            search_url = f"https://pubmed.ncbi.nlm.nih.gov/?term={encoded_query}"
            crawl_method = "PubMed Direct Search"
            search_info = "æœç´¢ç»“æœæ•°: é»˜è®¤ (é€šå¸¸20æ¡)"
        else:
            return f"âŒ ä¸æ”¯æŒçš„å­¦æœ¯æ•°æ®æº: {source}\næ”¯æŒçš„æ•°æ®æº: google_scholar, arxiv, pubmed"
        
        # ğŸ†• ç›´æ¥ä½¿ç”¨éšèº«æ¨¡å¼ï¼Œæé«˜å­¦æœ¯ç½‘ç«™çˆ¬å–æˆåŠŸç‡
        try:
            result = await crawl_stealth(search_url)
            crawl_method += " (Stealth Mode)"
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–å†…å®¹
            if "å¤±è´¥" in result or "Error" in result or "Sorry" in result:
                print(f"âš ï¸ éšèº«æ¨¡å¼é‡åˆ°é—®é¢˜ï¼Œå°è¯•æ™ºèƒ½çˆ¬å–æ¨¡å¼")
                # å›é€€åˆ°æ™ºèƒ½çˆ¬å–æ¨¡å¼
                result = await crawl_with_intelligence(
                    url=search_url,
                    crawl_mode="smart"
                )
                crawl_method = crawl_method.replace(" (Stealth Mode)", " (Intelligence Fallback)")
            
        except Exception as stealth_error:
            print(f"âš ï¸ éšèº«æ¨¡å¼å¤±è´¥ï¼Œå›é€€åˆ°æ™ºèƒ½çˆ¬å–æ¨¡å¼: {stealth_error}")
            # å›é€€åˆ°æ™ºèƒ½çˆ¬å–æ¨¡å¼
            result = await crawl_with_intelligence(
                url=search_url,
                crawl_mode="smart"
            )
            crawl_method += " (Intelligence Fallback)"
        
        # ä¸ºå­¦æœ¯æœç´¢ç»“æœæ·»åŠ ç‰¹æ®Šæ ‡è¯†å’Œæ ¼å¼åŒ–
        academic_header = f"""# ğŸ“ å­¦æœ¯æœç´¢ç»“æœ

**æŸ¥è¯¢**: {query}
**æ•°æ®æº**: {source.replace('_', ' ').title()}
**çˆ¬å–æ–¹å¼**: {crawl_method}
**{search_info}**
**çˆ¬å–æ¨¡å¼**: Stealth (éšèº«æ¨¡å¼ä¼˜å…ˆ)
**åŒ…å«æ‘˜è¦**: {'æ˜¯' if include_abstracts else 'å¦'}
**æœç´¢URL**: {search_url}

ğŸ’¡ **ä½¿ç”¨æç¤º**:
- å½“å‰è¿”å›æœç´¢ç»“æœé¡µé¢çš„åŸºæœ¬ä¿¡æ¯ï¼ˆè®ºæ–‡æ ‡é¢˜ã€ä½œè€…ã€é“¾æ¥ç­‰ï¼‰
- å¦‚éœ€è·å–è®ºæ–‡è¯¦ç»†å†…å®¹ï¼Œè¯·ä½¿ç”¨: crawl_with_intelligence(url, "deep", {deep_crawl_count})
- å…¶ä¸­ url å¯ä»¥æ˜¯ä¸Šé¢æœç´¢ç»“æœä¸­çš„å…·ä½“è®ºæ–‡é“¾æ¥

---

"""
        
        return academic_header + result
        
    except Exception as e:
        return f"âŒ å­¦æœ¯æœç´¢å¤±è´¥: {str(e)}"

# ===== å®éªŒæ€§åŠŸèƒ½ =====

@mcp.tool()
async def experimental_claude_analysis(
    content: str,
    analysis_type: str = "general",
    enable_claude: bool = False
) -> str:
    """
    Experimental AI-powered content analysis using Claude 3.7 (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Args:
        content: Content to analyze
        analysis_type: Analysis type (general/technical/academic/business)
        enable_claude: Must be explicitly set to True to call Claude API
        
    Returns:
        AI-powered content analysis results
        
    Use cases:
        - Advanced content summarization
        - Technical document analysis
        - Academic paper insights
        
    Warning:
        This is an experimental feature requiring Claude API configuration.
        Only calls external API when enable_claude=True.
    """
    
    if not enable_claude:
        return """Claude analysis feature not enabled
        
This is an experimental feature that requires:
1. Explicitly set enable_claude=True
2. Configure Claude API key
3. Confirm use of external API service

To enable, use:
experimental_claude_analysis(content="your content", enable_claude=True)
"""
    
    try:
        global config
        
        # Check Claude configuration
        claude_config_path = Path("v9_config/claude_config.json")
        if not claude_config_path.exists():
            return "Claude configuration file not found, please configure Claude API first"
        
        with open(claude_config_path, 'r', encoding='utf-8') as f:
            claude_config = json.load(f)
        
        if not claude_config.get("claude_api", {}).get("enabled", False):
            return "Claude API not enabled, please enable in configuration file"
        
        api_key = claude_config.get("claude_api", {}).get("api_key", "")
        if not api_key:
            return "Claude API key not configured"
        
        # ä½¿ç”¨é…ç½®ç®¡ç†å™¨çš„é¢„è§ˆé™åˆ¶
        preview_limit = config.content_limits.claude_preview_limit
        content_preview = content[:preview_limit] if len(content) > preview_limit else content
        
        # Here you can add actual Claude API call logic
        # Currently returning mock results
        return f"""Claude Analysis Results (Experimental)

Content Preview ({preview_limit} chars): {content_preview}{'...' if len(content) > preview_limit else ''}
Analysis Type: {analysis_type}
Status: Experimental feature, Claude API call logic to be implemented

Note: This feature is currently in development, actual Claude API integration is being improved.
Preview limit can be adjusted with: configure_crawl_settings("update", "content_limits", claude_preview_limit=200)
"""
        
    except Exception as e:
        return f"Claude analysis failed: {str(e)}"

# ===== ç³»ç»ŸçŠ¶æ€å’Œä¿¡æ¯å·¥å…· =====

@mcp.tool()
async def system_status() -> str:
    """
    Display system status and available tools information (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Returns:
        Current system status, version info, and tool availability
        
    Use cases:
        - Check system health
        - View available tools
        - Verify server configuration
    """
    try:
        global config
        
        # æ£€æµ‹å½“å‰Pythonç‰ˆæœ¬
        current_python = f"Python {sys.version.split()[0]}"
        
        # æ£€æµ‹è™šæ‹Ÿç¯å¢ƒçŠ¶æ€
        venv_status = "æœªæ¿€æ´»"
        venv_path = Path(__file__).parent.absolute() / ".venv"
        if venv_path.exists() and os.environ.get('VIRTUAL_ENV'):
            venv_status = f"å·²æ¿€æ´» ({venv_path})"
        
        status_info = f"""Context Scraper MCP Server V9

Version: 9.0.0
Status: Active
Python: {current_python}
Virtual Environment: {venv_status}
Enhancement: Unified Configuration Management + User Configurable Parameters + Academic Search
Total Tools: 11

Available Tools:
â€¢ crawl - Basic webpage crawling (é…ç½®åŒ–)
â€¢ crawl_with_intelligence - Smart crawling with optimization (é…ç½®åŒ– + æ·±åº¦æœç´¢)
â€¢ crawl_stealth - Anti-detection crawling (é…ç½®åŒ–)
â€¢ crawl_with_retry - Retry mechanism for unstable sites (é…ç½®åŒ–)
â€¢ crawl_with_geolocation - Geographic location spoofing (é…ç½®åŒ–)
â€¢ academic_search - Academic paper search and extraction (ğŸ†• NEW)
â€¢ experimental_claude_analysis - AI content analysis (é…ç½®åŒ–)
â€¢ configure_crawl_settings - é…ç½®ç®¡ç†å·¥å…·
â€¢ quick_config_content_limit - å¿«é€Ÿè®¾ç½®å†…å®¹é™åˆ¶
â€¢ quick_config_word_threshold - å¿«é€Ÿè®¾ç½®è¯æ•°é˜ˆå€¼
â€¢ system_status - Display system information

V9 New Features:
- âœ… Unified configuration management
- ğŸ”§ User-configurable parameters
- ğŸ“¦ Dynamic Python version detection (æ”¯æŒæ‰€æœ‰ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬æœªæ¥ç‰ˆæœ¬)
- ğŸš€ No manual 'source .venv/bin/activate' required
- ğŸ” Smart lib directory scanning
- ğŸ“‹ Detailed startup diagnostics
- âš™ï¸ Real-time configuration updates
- ğŸ“ Academic search with deep content extraction (ğŸ†• NEW)
- ğŸŒ Deep search for comprehensive web results (ğŸ†• NEW)

Current Configuration:
- ğŸ“„ Content Display Limit: {config.content_limits.markdown_display_limit} chars
- ğŸ¯ Word Count Threshold: {config.quality_control.word_count_threshold} words
- â±ï¸ Page Timeout: {config.timing_control.page_timeout_ms}ms
- ğŸ”„ Max Retries: {config.retry_control.max_retries}
- ğŸ‘¤ Show Word Count: {config.user_preferences.show_word_count}
- ğŸ‘¤ Show Detailed Logs: {config.user_preferences.show_detailed_logs}

Configuration Management:
- ğŸ”§ View all settings: configure_crawl_settings("show", "all")
- âš¡ Quick content limit: quick_config_content_limit(5000)
- âš¡ Quick word threshold: quick_config_word_threshold(100)
- ğŸ”„ Reset to defaults: configure_crawl_settings("reset")

Academic Search Features:
- ğŸ“ Google Scholar integration: academic_search("query", "google_scholar", 5)
- ğŸ“„ arXiv paper search: academic_search("query", "arxiv", 3)
- ğŸ¥ PubMed medical literature: academic_search("query", "pubmed", 5)
- ğŸ” Deep content extraction from academic sources
- ğŸ“Š Automatic paper metadata extraction

Deep Search Features:
- ğŸŒ Extract content from search result links
- ğŸ“„ Crawl multiple pages for comprehensive results
- âš™ï¸ Configurable result count (1-10 results)
- ğŸ¯ Smart link filtering and validation

System: Ready for web crawling operations with unified configuration management and academic search capabilities"""

        return status_info
        
    except Exception as e:
        return f"System Status Error: {str(e)}"

# ===== å¯åŠ¨ä¿¡æ¯ =====

def show_v9_welcome():
    """Display V9 startup information"""
    global config
    
    print("Context Scraper V9 MCP Server")
    print("=" * 50)
    print("ğŸ†• V9 New Features:")
    print("   âœ… Unified configuration management")
    print("   ğŸ”§ User-configurable parameters")
    print("   ğŸ“¦ Dynamic Python version detection (æ”¯æŒæœªæ¥ç‰ˆæœ¬)")
    print("   ğŸš€ No manual activation required")
    print("   ğŸ” Smart lib directory scanning")
    print("   ğŸ“‹ Detailed startup diagnostics")
    print("   âš™ï¸ Real-time configuration updates")
    print("   ğŸ“ Academic search with deep content extraction (ğŸ†• NEW)")
    print("   ğŸŒ Deep search for comprehensive web results (ğŸ†• NEW)")
    print()
    print("Configuration Status:")
    print(f"   ğŸ“„ Content Limit: {config.content_limits.markdown_display_limit} chars")
    print(f"   ğŸ¯ Word Threshold: {config.quality_control.word_count_threshold} words")
    print(f"   â±ï¸ Page Timeout: {config.timing_control.page_timeout_ms}ms")
    print(f"   ğŸ‘¤ Show Word Count: {config.user_preferences.show_word_count}")
    print()
    print("Python Version Support:")
    print("   ğŸ¯ Current: All existing Python versions")
    print("   ğŸš€ Future-proof: Automatically detects new versions")
    print("   ğŸ”„ Dynamic: Scans .venv/lib/ for python* directories")
    print("   ğŸ“Š Priority: Uses latest available version")
    print()
    print("Main Features:")
    print("   - Smart web crawling and content extraction")
    print("   - Search guide and best practices")
    print("   - Intent analysis and strategy optimization")
    print("   - Experimental Claude analysis feature")
    print("   - Unified configuration management")
    print("   - ğŸ“ Academic search (Google Scholar, arXiv, PubMed)")
    print("   - ğŸŒ Deep search with content extraction")
    print("=" * 50)
    print("Usage Instructions:")
    print("   - smart_search_guide: Smart search guide with academic support")
    print("   - crawl_with_intelligence: Smart web crawling + deep search")
    print("   - academic_search: Specialized academic paper search")
    print("   - crawl_stealth: Stealth mode crawling")
    print("   - crawl_with_geolocation: Geolocation spoofing")
    print("   - crawl_with_retry: Retry mode crawling")
    print("   - experimental_claude_analysis: Claude analysis")
    print("   - configure_crawl_settings: Configuration management")
    print()
    print("Configuration Commands:")
    print("   - configure_crawl_settings('show', 'all'): View all settings")
    print("   - quick_config_content_limit(5000): Set content limit")
    print("   - quick_config_word_threshold(100): Set word threshold")
    print()
    print("Academic Search Examples:")
    print("   - academic_search('machine learning', 'google_scholar', 5)")
    print("   - academic_search('transformer', 'arxiv', 3)")
    print("   - academic_search('COVID-19', 'pubmed', 5)")
    print()
    print("Deep Search Examples:")
    print("   - crawl_with_intelligence('https://google.com/search?q=AI', 'deep', 5)")
    print("   - crawl_with_intelligence('https://scholar.google.com/...', 'deep', 3)")
    print()
    print("Search Function:")
    print("   Use smart_search_guide to get intelligent search guidance")
    print("   AI will automatically recommend academic_search for research queries")
    print("=" * 50)

if __name__ == "__main__":
    show_v9_welcome()
