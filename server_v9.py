#!/usr/bin/env python3
# server_v9.py - Context Scraper MCP Server V9
# Enhanced version with unified configuration management and user-configurable parameters

import sys
import os
from pathlib import Path

# ===== è‡ªåŠ¨æ¿€æ´»è™šæ‹Ÿç¯å¢ƒåŠŸèƒ½ =====
def activate_virtual_environment():
    """
    è‡ªåŠ¨æ¿€æ´»å½“å‰é¡¹ç›®çš„è™šæ‹Ÿç¯å¢ƒ
    è¿™ä¸ªå‡½æ•°ä¼šåœ¨å¯¼å…¥å…¶ä»–æ¨¡å—ä¹‹å‰æ‰§è¡Œï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„Pythonç¯å¢ƒ
    æ”¯æŒæ‰€æœ‰Pythonç‰ˆæœ¬ï¼ŒåŒ…æ‹¬æœªæ¥ç‰ˆæœ¬
    """
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_dir = Path(__file__).parent.absolute()
    venv_path = current_dir / ".venv"
    
    print(f"ğŸ” æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ: {venv_path}")
    
    if venv_path.exists():
        print(f"âœ… æ‰¾åˆ°è™šæ‹Ÿç¯å¢ƒç›®å½•: {venv_path}")
        
        # åŠ¨æ€æ£€æµ‹æ‰€æœ‰å¯ç”¨çš„Pythonç‰ˆæœ¬
        lib_path = venv_path / "lib"
        site_packages_path = None
        detected_version = None
        
        if lib_path.exists():
            print(f"ğŸ” æ‰«ælibç›®å½•: {lib_path}")
            
            # è·å–æ‰€æœ‰python*ç›®å½•ï¼Œè‡ªåŠ¨æ”¯æŒæœªæ¥ç‰ˆæœ¬
            python_dirs = []
            for item in lib_path.iterdir():
                if item.is_dir() and item.name.startswith('python'):
                    python_dirs.append(item.name)
            
            # æŒ‰ç‰ˆæœ¬å·æ’åºï¼Œä¼˜å…ˆä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
            python_dirs.sort(reverse=True)
            print(f"ğŸ” å‘ç°Pythonç‰ˆæœ¬: {python_dirs}")
            
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ…å«site-packagesçš„ç‰ˆæœ¬
            for py_version in python_dirs:
                potential_path = lib_path / py_version / "site-packages"
                print(f"ğŸ” æ£€æŸ¥è·¯å¾„: {potential_path}")
                if potential_path.exists():
                    site_packages_path = potential_path
                    detected_version = py_version
                    print(f"âœ… æ‰¾åˆ°å¯ç”¨çš„Pythonç‰ˆæœ¬: {py_version}")
                    break
        
        if site_packages_path:
            # å°†è™šæ‹Ÿç¯å¢ƒçš„ site-packages æ·»åŠ åˆ° sys.path çš„æœ€å‰é¢
            site_packages_str = str(site_packages_path)
            if site_packages_str not in sys.path:
                sys.path.insert(0, site_packages_str)
                print(f"âœ… è™šæ‹Ÿç¯å¢ƒå·²è‡ªåŠ¨æ¿€æ´»!")
                print(f"ğŸ“¦ Pythonç‰ˆæœ¬: {detected_version}")
                print(f"ğŸ“¦ Site-packagesè·¯å¾„: {site_packages_str}")
            else:
                print(f"â„¹ï¸  è™šæ‹Ÿç¯å¢ƒå·²åœ¨sys.pathä¸­")
                print(f"ğŸ“¦ å½“å‰Pythonç‰ˆæœ¬: {detected_version}")
            
            # è®¾ç½®è™šæ‹Ÿç¯å¢ƒç›¸å…³çš„ç¯å¢ƒå˜é‡
            os.environ['VIRTUAL_ENV'] = str(venv_path)
            
            # æ›´æ–°PATHç¯å¢ƒå˜é‡ï¼Œç¡®ä¿ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒçš„å¯æ‰§è¡Œæ–‡ä»¶
            venv_bin = venv_path / "bin"
            if venv_bin.exists():
                current_path = os.environ.get('PATH', '')
                if str(venv_bin) not in current_path:
                    os.environ['PATH'] = f"{venv_bin}:{current_path}"
                    print(f"ğŸ”§ PATHå·²æ›´æ–°ï¼Œä¼˜å…ˆä½¿ç”¨è™šæ‹Ÿç¯å¢ƒçš„å¯æ‰§è¡Œæ–‡ä»¶")
                else:
                    print(f"â„¹ï¸  è™šæ‹Ÿç¯å¢ƒbinç›®å½•å·²åœ¨PATHä¸­")
        else:
            print(f"âš ï¸  è™šæ‹Ÿç¯å¢ƒå­˜åœ¨ä½†æœªæ‰¾åˆ°site-packagesç›®å½•")
            if lib_path.exists():
                print(f"ğŸ“ libç›®å½•å†…å®¹:")
                for item in lib_path.iterdir():
                    print(f"   - {item.name} ({'ç›®å½•' if item.is_dir() else 'æ–‡ä»¶'})")
            else:
                print(f"ğŸ“ libç›®å½•ä¸å­˜åœ¨: {lib_path}")
    else:
        print(f"âš ï¸  è™šæ‹Ÿç¯å¢ƒç›®å½•ä¸å­˜åœ¨: {venv_path}")
        print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿å·²åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (.venv)")
        print("ğŸ’¡ åˆ›å»ºå‘½ä»¤: uv sync æˆ– python -m venv .venv")

# åœ¨å¯¼å…¥ä»»ä½•å…¶ä»–æ¨¡å—ä¹‹å‰æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
print("ğŸš€ æ­£åœ¨å¯åŠ¨ Context Scraper MCP Server V9...")
activate_virtual_environment()

# ===== å¯¼å…¥ä¾èµ–æ¨¡å— =====

import asyncio
import json
import time
from typing import Optional, List, Dict, Any

# V9 core components
from v9_core.intent_analyzer import analyze_user_intent, UserIntent, SearchEngineIntent, IntentType
from v9_core.crawl_config_manager import get_crawl_config, reload_crawl_config

# Crawl4AI components
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from mcp.server.fastmcp import Context, FastMCP

# åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
config = get_crawl_config()
print(f"âš™ï¸  é…ç½®ç®¡ç†å™¨å·²åˆå§‹åŒ–")

# Create MCP server
mcp = FastMCP("ContextScraperV9")

# ===== é…ç½®ç®¡ç†å·¥å…· =====

@mcp.tool()
async def configure_crawl_settings(
    action: str = "show",
    setting_type: str = "all",
    **kwargs
) -> str:
    """
    é…ç½®çˆ¬å–å‚æ•°è®¾ç½®
    
    Args:
        action: æ“ä½œç±»å‹ (show/update/reset)
        setting_type: è®¾ç½®ç±»å‹ (content_limits/quality_control/timing_control/user_preferences/all)
        **kwargs: å…·ä½“çš„é…ç½®å‚æ•°
        
    Returns:
        é…ç½®æ“ä½œç»“æœ
        
    Use cases:
        - æŸ¥çœ‹å½“å‰é…ç½®: configure_crawl_settings("show", "all")
        - æ›´æ–°å†…å®¹é™åˆ¶: configure_crawl_settings("update", "content_limits", markdown_display_limit=5000)
        - æ›´æ–°ç”¨æˆ·åå¥½: configure_crawl_settings("update", "user_preferences", show_word_count=False)
    """
    
    try:
        global config
        
        if action == "show":
            if setting_type == "all":
                return config.get_config_summary()
            elif setting_type == "content_limits":
                return f"""ğŸ“„ å†…å®¹é™åˆ¶é…ç½®:
- Markdownæ˜¾ç¤ºé™åˆ¶: {config.content_limits.markdown_display_limit} å­—ç¬¦
- Claudeé¢„è§ˆé™åˆ¶: {config.content_limits.claude_preview_limit} å­—ç¬¦
- åŸºç¡€çˆ¬å–æ— é™åˆ¶: {config.content_limits.basic_crawl_unlimited}"""
            elif setting_type == "quality_control":
                return f"""ğŸ¯ è´¨é‡æ§åˆ¶é…ç½®:
- è¯æ•°é˜ˆå€¼: {config.quality_control.word_count_threshold} è¯
- æœ€å°è´¨é‡: {config.quality_control.min_content_quality}"""
            elif setting_type == "timing_control":
                return f"""â±ï¸ æ—¶é—´æ§åˆ¶é…ç½®:
- é¡µé¢è¶…æ—¶: {config.timing_control.page_timeout_ms}ms
- éšèº«å»¶è¿Ÿ: {config.timing_control.stealth_delay_seconds}s
- åŠ¨æ€å†…å®¹å»¶è¿Ÿ: {config.timing_control.dynamic_content_delay_seconds}s"""
            elif setting_type == "user_preferences":
                return f"""ğŸ‘¤ ç”¨æˆ·åå¥½è®¾ç½®:
- è¯¦ç»†æ—¥å¿—: {config.user_preferences.show_detailed_logs}
- æ˜¾ç¤ºè¯æ•°: {config.user_preferences.show_word_count}
- æ˜¾ç¤ºæ—¶é—´: {config.user_preferences.show_timing_info}
- ç´§å‡‘è¾“å‡º: {config.user_preferences.compact_output}"""
        
        elif action == "update":
            if setting_type == "content_limits":
                config.update_content_limits(**kwargs)
                return f"âœ… å†…å®¹é™åˆ¶é…ç½®å·²æ›´æ–°: {kwargs}"
            elif setting_type == "quality_control":
                config.update_quality_control(**kwargs)
                return f"âœ… è´¨é‡æ§åˆ¶é…ç½®å·²æ›´æ–°: {kwargs}"
            elif setting_type == "timing_control":
                config.update_timing_control(**kwargs)
                return f"âœ… æ—¶é—´æ§åˆ¶é…ç½®å·²æ›´æ–°: {kwargs}"
            elif setting_type == "user_preferences":
                config.update_user_preferences(**kwargs)
                return f"âœ… ç”¨æˆ·åå¥½è®¾ç½®å·²æ›´æ–°: {kwargs}"
        
        elif action == "reset":
            config = reload_crawl_config()
            return "âœ… é…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼"
        
        return f"âŒ ä¸æ”¯æŒçš„æ“ä½œ: action={action}, setting_type={setting_type}"
        
    except Exception as e:
        return f"âŒ é…ç½®æ“ä½œå¤±è´¥: {str(e)}"

@mcp.tool()
async def quick_config_content_limit(limit: int = 3000) -> str:
    """
    å¿«é€Ÿè®¾ç½®å†…å®¹æ˜¾ç¤ºé™åˆ¶
    
    Args:
        limit: å†…å®¹æ˜¾ç¤ºå­—ç¬¦æ•°é™åˆ¶
        
    Returns:
        è®¾ç½®ç»“æœ
        
    Use cases:
        - è®¾ç½®5000å­—ç¬¦é™åˆ¶: quick_config_content_limit(5000)
        - è®¾ç½®1000å­—ç¬¦é™åˆ¶: quick_config_content_limit(1000)
    """
    try:
        global config
        config.update_content_limits(markdown_display_limit=limit)
        return f"âœ… å†…å®¹æ˜¾ç¤ºé™åˆ¶å·²è®¾ç½®ä¸º: {limit} å­—ç¬¦"
    except Exception as e:
        return f"âŒ è®¾ç½®å¤±è´¥: {str(e)}"

@mcp.tool()
async def quick_config_word_threshold(threshold: int = 50) -> str:
    """
    å¿«é€Ÿè®¾ç½®è¯æ•°é˜ˆå€¼
    
    Args:
        threshold: æœ€å°è¯æ•°é˜ˆå€¼
        
    Returns:
        è®¾ç½®ç»“æœ
        
    Use cases:
        - è®¾ç½®100è¯é˜ˆå€¼: quick_config_word_threshold(100)
        - è®¾ç½®20è¯é˜ˆå€¼: quick_config_word_threshold(20)
    """
    try:
        global config
        config.update_quality_control(word_count_threshold=threshold)
        return f"âœ… è¯æ•°é˜ˆå€¼å·²è®¾ç½®ä¸º: {threshold} è¯"
    except Exception as e:
        return f"âŒ è®¾ç½®å¤±è´¥: {str(e)}"

# ===== è¾…åŠ©å‡½æ•° =====

def format_crawl_result(result, url: str, tool_name: str, extra_info: Dict[str, Any] = None) -> str:
    """
    æ ¼å¼åŒ–çˆ¬å–ç»“æœï¼Œä½¿ç”¨é…ç½®ç®¡ç†å™¨çš„è®¾ç½®
    
    Args:
        result: çˆ¬å–ç»“æœå¯¹è±¡
        url: ç›®æ ‡URL
        tool_name: å·¥å…·åç§°
        extra_info: é¢å¤–ä¿¡æ¯å­—å…¸
        
    Returns:
        æ ¼å¼åŒ–çš„ç»“æœå­—ç¬¦ä¸²
    """
    global config
    
    if not result.success:
        return f"{tool_name} å¤±è´¥\n\nURL: {url}\nError: {result.error_message}"
    
    # åŸºç¡€ä¿¡æ¯
    response = f"{tool_name} æˆåŠŸ\n\nURL: {url}\n"
    
    # æ ‡é¢˜ä¿¡æ¯
    title = result.metadata.get('title', 'Unknown')
    response += f"Title: {title}\n"
    
    # è¯æ•°ç»Ÿè®¡ï¼ˆæ ¹æ®ç”¨æˆ·åå¥½ï¼‰
    if config.user_preferences.show_word_count:
        word_count = len(result.markdown.split()) if result.markdown else 0
        response += f"Word Count: {word_count}\n"
    
    # é¢å¤–ä¿¡æ¯
    if extra_info:
        for key, value in extra_info.items():
            response += f"{key}: {value}\n"
    
    # å†…å®¹æ˜¾ç¤º
    if result.markdown:
        if config.content_limits.basic_crawl_unlimited and tool_name == "Basic Crawl":
            # åŸºç¡€çˆ¬å–æ˜¾ç¤ºå®Œæ•´å†…å®¹
            response += f"\nContent:\n\n{result.markdown}"
        else:
            # å…¶ä»–å·¥å…·ä½¿ç”¨é…ç½®çš„é™åˆ¶
            limit = config.content_limits.markdown_display_limit
            if len(result.markdown) > limit:
                response += f"\nContent (å‰{limit}å­—ç¬¦):\n\n{result.markdown[:limit]}..."
            else:
                response += f"\nContent:\n\n{result.markdown}"
    else:
        response += "\nContent: æ— å†…å®¹"
    
    return response

def get_crawler_config(tool_type: str = "default") -> CrawlerRunConfig:
    """
    æ ¹æ®å·¥å…·ç±»å‹è·å–çˆ¬å–é…ç½®
    
    Args:
        tool_type: å·¥å…·ç±»å‹ (default/stealth/geolocation/retry/intelligence)
        
    Returns:
        é…ç½®å¥½çš„CrawlerRunConfigå¯¹è±¡
    """
    global config
    
    # åŸºç¡€é…ç½®
    cache_mode = CacheMode.BYPASS if config.cache_control.default_cache_mode == "BYPASS" else CacheMode.ENABLED
    
    base_config = {
        "cache_mode": cache_mode,
        "word_count_threshold": config.quality_control.word_count_threshold,
    }
    
    # æ ¹æ®å·¥å…·ç±»å‹æ·»åŠ ç‰¹å®šé…ç½®
    if tool_type == "default":
        base_config.update({
            "page_timeout": config.timing_control.page_timeout_ms,
            "wait_until": config.browser_control.default_wait_until
        })
    elif tool_type == "stealth":
        base_config.update({
            "delay_before_return_html": config.timing_control.stealth_delay_seconds
        })
    elif tool_type == "intelligence":
        # æ™ºèƒ½æ¨¡å¼æ ¹æ®å†…å®¹ç±»å‹åŠ¨æ€è®¾ç½®å»¶è¿Ÿ
        base_config.update({
            "delay_before_return_html": config.timing_control.dynamic_content_delay_seconds
        })
    
    return CrawlerRunConfig(**base_config)

# ===== V6 Core Features =====

@mcp.prompt()
def smart_search_guide(search_query: str = "What do you want to search?") -> str:
    """Smart Search Guide - AI-driven search tool selection assistant
    
    Intelligently recommends the most suitable crawling tools based on search content,
    provides decision trees and specific execution suggestions.
    """
    
    # URL encoding
    import urllib.parse
    encoded_query = urllib.parse.quote_plus(search_query)
    
    # Build search engine URLs
    google_url = f"https://www.google.com/search?q={encoded_query}"
    baidu_url = f"https://www.baidu.com/s?wd={encoded_query}"
    bing_url = f"https://www.bing.com/search?q={encoded_query}"
    duckduckgo_url = f"https://duckduckgo.com/?q={encoded_query}"
    
    # Smart analysis of search content
    query_lower = search_query.lower()
    
    # Detect language and content type
    has_chinese = any('\u4e00' <= char <= '\u9fff' for char in search_query)
    
    # Technical keywords (expanded)
    technical_keywords = [
        'api', 'code', 'programming', 'github', 'stackoverflow', 'python', 'javascript', 'java', 'react', 'vue',
        'docker', 'kubernetes', 'aws', 'cloud', 'database', 'sql', 'nosql', 'mongodb', 'redis', 'nginx',
        'linux', 'ubuntu', 'centos', 'bash', 'shell', 'git', 'devops', 'ci/cd', 'jenkins', 'terraform',
        'machine learning', 'ai', 'deep learning', 'tensorflow', 'pytorch', 'data science', 'algorithm',
        'framework', 'library', 'sdk', 'compiler', 'debugger', 'ide', 'vscode', 'intellij',
        'ç¼–ç¨‹', 'ä»£ç ', 'å¼€å‘', 'æŠ€æœ¯', 'ç®—æ³•', 'æ•°æ®åº“', 'æœåŠ¡å™¨', 'äº‘è®¡ç®—', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ '
    ]
    
    # Academic keywords (expanded)
    academic_keywords = [
        'research', 'paper', 'study', 'journal', 'publication', 'thesis', 'dissertation', 'conference',
        'academic', 'scholar', 'university', 'college', 'professor', 'phd', 'master', 'bachelor',
        'citation', 'bibliography', 'peer review', 'methodology', 'analysis', 'experiment', 'survey',
        'ç ”ç©¶', 'è®ºæ–‡', 'å­¦æœ¯', 'æœŸåˆŠ', 'ä¼šè®®', 'å¤§å­¦', 'å­¦è€…', 'åšå£«', 'ç¡•å£«', 'å®éªŒ', 'è°ƒç ”'
    ]
    
    # News keywords (expanded)
    news_keywords = [
        'news', 'latest', 'breaking', 'update', 'report', 'announcement', 'press release', 'headline',
        'current', 'today', 'yesterday', 'recent', 'happening', 'event', 'incident', 'story',
        'æ–°é—»', 'æœ€æ–°', 'ä»Šæ—¥', 'æ˜¨æ—¥', 'æœ€è¿‘', 'äº‹ä»¶', 'æŠ¥é“', 'æ¶ˆæ¯', 'å¤´æ¡', 'å¿«è®¯'
    ]
    
    # Privacy keywords (expanded)
    privacy_keywords = [
        'privacy', 'anonymous', 'private', 'secure', 'confidential', 'hidden', 'secret', 'vpn',
        'tor', 'encryption', 'security', 'protect', 'safe', 'incognito', 'stealth',
        'éšç§', 'åŒ¿å', 'ç§å¯†', 'å®‰å…¨', 'ä¿æŠ¤', 'åŠ å¯†', 'ç§˜å¯†', 'éšèº«'
    ]
    
    is_technical = any(keyword in query_lower for keyword in technical_keywords)
    is_academic = any(keyword in query_lower for keyword in academic_keywords)
    is_news = any(keyword in query_lower for keyword in news_keywords)
    is_sensitive = any(keyword in query_lower for keyword in privacy_keywords)
    
    # Smart recommendation with priority logic
    if is_sensitive:
        # Privacy is highest priority
        primary_recommendation = "DuckDuckGo Search"
        primary_url = duckduckgo_url
        primary_reason = "Privacy-sensitive content detected, DuckDuckGo doesn't track users"
    elif is_technical:
        # Technical content gets Google
        primary_recommendation = "Google Search"
        primary_url = google_url
        primary_reason = "Technical content detected, Google has the richest technical resources"
    elif is_academic:
        # Academic content gets Google
        primary_recommendation = "Google Search"
        primary_url = google_url
        primary_reason = "Academic content detected, Google Scholar and academic resources are more comprehensive"
    else:
        # Default to Google for general English content
        primary_recommendation = "Google Search"
        primary_url = google_url
        primary_reason = "General search, Google has the widest coverage and best algorithms"
    
    # Build feature analysis display
    features = []
    if has_chinese:
        features.append("Chinese")
    else:
        features.append("English")
    
    if is_technical:
        features.append("Technical")
    if is_academic:
        features.append("Academic")
    if is_news:
        features.append("News")
    if is_sensitive:
        features.append("Privacy-Sensitive")
    
    if len(features) == 1:  # Only language detected
        features.append("General")
    
    features_display = " | ".join(features)
    
    # è·å–å½“å‰é…ç½®ä¿¡æ¯
    global config
    content_limit = config.content_limits.markdown_display_limit
    
    return f"""# ğŸ” AI Smart Search Assistant (V9 é…ç½®å¢å¼ºç‰ˆ)

## ğŸ“Š Search Analysis
**Query**: {search_query}
**Content Features**: {features_display}
**Confidence**: {'High' if len([f for f in [is_technical, is_academic, is_news, is_sensitive] if f]) > 0 else 'Medium'}
**Current Content Limit**: {content_limit} å­—ç¬¦

## ğŸ¯ AI Recommended Solution (Use First)

### â­ Recommended: {primary_recommendation}
**Analysis**: {primary_reason}

**ğŸš€ Execute Now**:
```
crawl_with_intelligence(
    url="{primary_url}",
    use_smart_analysis=True
)
```

## Other Options

### Basic Search Options
| Engine | Use Case | Command |
|--------|----------|---------|
| Google | Technical, academic, international | `crawl_with_intelligence("{google_url}", True)` |
| Baidu | Chinese content, local info | `crawl_with_intelligence("{baidu_url}", True)` |
| Bing | Microsoft ecosystem, business | `crawl_with_intelligence("{bing_url}", True)` |
| DuckDuckGo | Privacy protection, anonymous | `crawl_with_intelligence("{duckduckgo_url}", True)` |

### Anti-Detection Options (Use when blocked)
| Tool | Use Case | Command |
|------|----------|---------|
| Stealth Mode | Basic anti-bot detection | `crawl_stealth("{primary_url}")` |
| Geo Spoofing | Regional content restrictions | `crawl_with_geolocation("{primary_url}", "random")` |
| Retry Mode | Unstable websites | `crawl_with_retry("{primary_url}")` |

## ğŸ”§ Configuration Options

### Quick Config Commands
- è°ƒæ•´å†…å®¹é™åˆ¶: `quick_config_content_limit(5000)`
- è°ƒæ•´è¯æ•°é˜ˆå€¼: `quick_config_word_threshold(100)`
- æŸ¥çœ‹æ‰€æœ‰é…ç½®: `configure_crawl_settings("show", "all")`

## Decision Flow

```
Start Search
    â†“
Use AI Recommendation
    â†“
Success? â†’ Yes â†’ Done âœ…
    â†“
    No
    â†“
Try Stealth Mode
    â†“
Success? â†’ Yes â†’ Done âœ…
    â†“
    No
    â†“
Try Geo Spoofing
    â†“
Success? â†’ Yes â†’ Done âœ…
    â†“
    No
    â†“
Use Retry Mode â†’ Done âœ…
```

## ğŸ’¡ Quick Start

1. **ğŸ¯ Direct Use**: Copy the AI recommended command above
2. **ğŸ”„ If Problems**: Follow decision flow step by step  
3. **ğŸ“Š Compare Results**: Use different engines for comparison
4. **ğŸ›¡ï¸ If Blocked**: Try anti-detection options
5. **âš™ï¸ Customize**: Use config commands to adjust settings

## ğŸ“‹ Pro Tips

- **First try**: Always start with AI recommendation
- **General searches**: Google usually works best
- **Privacy matters**: Use DuckDuckGo for sensitive topics
- **Getting blocked**: Try stealth mode or geo spoofing
- **Unstable sites**: Use retry mode with multiple attempts
- **Content too long**: Use `quick_config_content_limit()` to adjust

## âš ï¸ Important Notes

- Use reasonable delays between requests
- Some websites may still block automated access
- Current settings can be viewed with `configure_crawl_settings("show")`

**ğŸš€ Ready to search? Start with the AI recommended solution above!**
"""
# ===== é…ç½®åŒ–çˆ¬å–å·¥å…· =====

@mcp.tool()
async def crawl(url: str) -> str:
    """
    Basic webpage crawling with Markdown conversion (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Args:
        url: Target webpage URL
        
    Returns:
        Webpage content in Markdown format
        
    Use cases:
        - Simple content extraction
        - Static webpage crawling  
        - Quick content preview
    """
    try:
        global config
        
        browser_config = BrowserConfig(
            headless=config.browser_control.headless_mode,
            browser_type=config.browser_control.browser_type
        )
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            crawl_config = get_crawler_config("default")
            result = await crawler.arun(url=url, config=crawl_config)
            
            return format_crawl_result(result, url, "Basic Crawl")
                
    except Exception as e:
        return f"Basic Crawl Error\n\nURL: {url}\nException: {str(e)}"

@mcp.tool()
async def crawl_stealth(url: str) -> str:
    """
    Stealth web crawling with anti-detection techniques (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Args:
        url: Target webpage URL
        
    Returns:
        Webpage content crawled with stealth techniques
        
    Use cases:
        - Bypass anti-bot protection
        - Avoid rate limiting detection
        - Privacy-focused crawling
    """
    
    try:
        global config
        
        # Import anti-detection functionality
        import sys
        sys.path.append('legacy/servers')
        from anti_detection import create_stealth_config
        
        # Use stealth configuration
        browser_config = create_stealth_config()
        crawl_config = get_crawler_config("stealth")
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=crawl_config) 
            
            if result.success:
                # Show disguise information
                ua_info = browser_config.user_agent[:80] + "..." if len(browser_config.user_agent) > 80 else browser_config.user_agent
                viewport_info = f"{browser_config.viewport_width}x{browser_config.viewport_height}"
                
                extra_info = {
                    "Disguised UA": ua_info,
                    "Viewport": viewport_info,
                    "Anti-Detection": "Enabled",
                    "Stealth Delay": f"{config.timing_control.stealth_delay_seconds}s"
                }
                
                return format_crawl_result(result, url, "Stealth Crawling", extra_info)
            else:
                return f"Stealth crawling failed: {result.error_message}"
                
    except ImportError:
        return "Anti-detection module not found, please check legacy/servers/anti_detection.py"
    except Exception as e:
        return f"Stealth crawling error: {str(e)}"

@mcp.tool()
async def crawl_with_geolocation(url: str, location: str = "random") -> str:
    """
    Geolocation spoofing crawl to bypass regional restrictions (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Args:
        url: Target webpage URL
        location: Geographic location (random/newyork/london/tokyo/paris/berlin/toronto/singapore/sydney)
        
    Returns:
        Webpage content with geolocation spoofing information
        
    Use cases:
        - Access region-locked content
        - Test location-based features
        - Bypass geographic restrictions
    """
    
    try:
        global config
        
        # Import geolocation spoofing functionality
        import sys
        sys.path.append('legacy/servers')
        from anti_detection import create_geo_spoofed_config
        
        # Create geolocation spoofing configuration
        browser_config, geo_config = create_geo_spoofed_config(location)
        crawl_config = get_crawler_config("geolocation")
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=crawl_config)
            
            if result.success:
                extra_info = {
                    "Spoofed Location": f"Lat {geo_config.latitude:.4f}, Lng {geo_config.longitude:.4f}",
                    "Accuracy": f"{geo_config.accuracy:.1f}m"
                }
                
                return format_crawl_result(result, url, "Geolocation Spoofing Crawl", extra_info)
            else:
                return f"Geolocation spoofing crawl failed: {result.error_message}"
                
    except ImportError:
        return "Geolocation spoofing module not found, please check legacy/servers/anti_detection.py"
    except Exception as e:
        return f"Geolocation spoofing crawl error: {str(e)}"

@mcp.tool()
async def crawl_with_retry(url: str, max_retries: Optional[int] = None) -> str:
    """
    Retry crawling with exponential backoff for unstable websites (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Args:
        url: Target webpage URL
        max_retries: Maximum number of retry attempts (å¦‚æœä¸æŒ‡å®šï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼)
        
    Returns:
        Webpage content with retry attempt information
        
    Use cases:
        - Handle unstable network connections
        - Retry temporarily unavailable websites
        - Overcome intermittent failures
    """
    
    try:
        global config
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é‡è¯•æ¬¡æ•°ï¼Œé™¤éç”¨æˆ·æ˜ç¡®æŒ‡å®š
        if max_retries is None:
            max_retries = config.retry_control.max_retries
        
        # Import retry manager
        import sys
        sys.path.append('legacy/servers')
        from anti_detection import create_retry_manager, create_stealth_config
        
        retry_manager = create_retry_manager(max_retries)
        browser_config = create_stealth_config()  # Use stealth mode to improve success rate
        crawl_config = get_crawler_config("retry")
        
        start_time = time.time()
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await retry_manager.execute_with_retry(crawler, url, crawl_config)
            
            elapsed_time = time.time() - start_time
            
            if result.success:
                extra_info = {
                    "Time Taken": f"{elapsed_time:.2f}s",
                    "Max Retries": str(max_retries),
                    "Stealth Mode": "Enabled"
                }
                
                return format_crawl_result(result, url, "Retry Crawling", extra_info)
            else:
                return f"Retry crawling failed: {result.error_message}"
                
    except ImportError:
        return "Retry management module not found, please check legacy/servers/anti_detection.py"
    except Exception as e:
        return f"Retry crawling error: {str(e)}"

@mcp.tool()
async def crawl_with_intelligence(
    url: str,
    use_smart_analysis: bool = True
) -> str:
    """
    Smart web crawling with content optimization and analysis (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Args:
        url: Target webpage URL
        use_smart_analysis: Enable intelligent content analysis and optimization
        
    Returns:
        Optimized webpage content in Markdown format
        
    Use cases:
        - Dynamic content websites
        - Complex page structures
        - Content-heavy websites requiring optimization
    """
    
    try:
        global config
        
        # Analyze URL intent
        if use_smart_analysis and config.advanced_settings.enable_smart_analysis:
            intent = analyze_user_intent(f"crawl {url}")
            
            # Adjust crawling strategy based on intent
            browser_config = BrowserConfig(
                headless=config.browser_control.headless_mode,
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            )
            
            # åŠ¨æ€è°ƒæ•´å»¶è¿Ÿæ—¶é—´
            delay_time = config.timing_control.dynamic_content_delay_seconds if intent.dynamic_content else config.timing_control.default_delay_seconds
            
            crawl_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS if config.cache_control.default_cache_mode == "BYPASS" else CacheMode.ENABLED,
                word_count_threshold=config.quality_control.word_count_threshold,
                delay_before_return_html=delay_time
            )
        else:
            browser_config = BrowserConfig(headless=config.browser_control.headless_mode)
            crawl_config = get_crawler_config("intelligence")
        
        # Execute crawling
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=crawl_config)
            
            if result.success:
                extra_info = {}
                if use_smart_analysis and config.advanced_settings.enable_smart_analysis:
                    extra_info["Smart Analysis"] = "Enabled"
                
                return format_crawl_result(result, url, "V9 Smart Crawling", extra_info)
            else:
                return f"Crawling failed: {result.error_message}"
                
    except Exception as e:
        return f"Crawling process error: {str(e)}"
# ===== å®éªŒæ€§åŠŸèƒ½ =====

@mcp.tool()
async def experimental_claude_analysis(
    content: str,
    analysis_type: str = "general",
    enable_claude: bool = False
) -> str:
    """
    Experimental AI-powered content analysis using Claude 3.7 (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Args:
        content: Content to analyze
        analysis_type: Analysis type (general/technical/academic/business)
        enable_claude: Must be explicitly set to True to call Claude API
        
    Returns:
        AI-powered content analysis results
        
    Use cases:
        - Advanced content summarization
        - Technical document analysis
        - Academic paper insights
        
    Warning:
        This is an experimental feature requiring Claude API configuration.
        Only calls external API when enable_claude=True.
    """
    
    if not enable_claude:
        return """Claude analysis feature not enabled
        
This is an experimental feature that requires:
1. Explicitly set enable_claude=True
2. Configure Claude API key
3. Confirm use of external API service

To enable, use:
experimental_claude_analysis(content="your content", enable_claude=True)
"""
    
    try:
        global config
        
        # Check Claude configuration
        claude_config_path = Path("v9_config/claude_config.json")
        if not claude_config_path.exists():
            return "Claude configuration file not found, please configure Claude API first"
        
        with open(claude_config_path, 'r', encoding='utf-8') as f:
            claude_config = json.load(f)
        
        if not claude_config.get("claude_api", {}).get("enabled", False):
            return "Claude API not enabled, please enable in configuration file"
        
        api_key = claude_config.get("claude_api", {}).get("api_key", "")
        if not api_key:
            return "Claude API key not configured"
        
        # ä½¿ç”¨é…ç½®ç®¡ç†å™¨çš„é¢„è§ˆé™åˆ¶
        preview_limit = config.content_limits.claude_preview_limit
        content_preview = content[:preview_limit] if len(content) > preview_limit else content
        
        # Here you can add actual Claude API call logic
        # Currently returning mock results
        return f"""Claude Analysis Results (Experimental)

Content Preview ({preview_limit} chars): {content_preview}{'...' if len(content) > preview_limit else ''}
Analysis Type: {analysis_type}
Status: Experimental feature, Claude API call logic to be implemented

Note: This feature is currently in development, actual Claude API integration is being improved.
Preview limit can be adjusted with: configure_crawl_settings("update", "content_limits", claude_preview_limit=200)
"""
        
    except Exception as e:
        return f"Claude analysis failed: {str(e)}"

# ===== ç³»ç»ŸçŠ¶æ€å’Œä¿¡æ¯å·¥å…· =====

@mcp.tool()
async def system_status() -> str:
    """
    Display system status and available tools information (é…ç½®åŒ–ç‰ˆæœ¬).
    
    Returns:
        Current system status, version info, and tool availability
        
    Use cases:
        - Check system health
        - View available tools
        - Verify server configuration
    """
    try:
        global config
        
        # æ£€æµ‹å½“å‰Pythonç‰ˆæœ¬
        current_python = f"Python {sys.version.split()[0]}"
        
        # æ£€æµ‹è™šæ‹Ÿç¯å¢ƒçŠ¶æ€
        venv_status = "æœªæ¿€æ´»"
        venv_path = Path(__file__).parent.absolute() / ".venv"
        if venv_path.exists() and os.environ.get('VIRTUAL_ENV'):
            venv_status = f"å·²æ¿€æ´» ({venv_path})"
        
        status_info = f"""Context Scraper MCP Server V9

Version: 9.0.0
Status: Active
Python: {current_python}
Virtual Environment: {venv_status}
Enhancement: Unified Configuration Management + User Configurable Parameters
Total Tools: 10

Available Tools:
â€¢ crawl - Basic webpage crawling (é…ç½®åŒ–)
â€¢ crawl_with_intelligence - Smart crawling with optimization (é…ç½®åŒ–)
â€¢ crawl_stealth - Anti-detection crawling (é…ç½®åŒ–)
â€¢ crawl_with_retry - Retry mechanism for unstable sites (é…ç½®åŒ–)
â€¢ crawl_with_geolocation - Geographic location spoofing (é…ç½®åŒ–)
â€¢ experimental_claude_analysis - AI content analysis (é…ç½®åŒ–)
â€¢ configure_crawl_settings - é…ç½®ç®¡ç†å·¥å…·
â€¢ quick_config_content_limit - å¿«é€Ÿè®¾ç½®å†…å®¹é™åˆ¶
â€¢ quick_config_word_threshold - å¿«é€Ÿè®¾ç½®è¯æ•°é˜ˆå€¼
â€¢ system_status - Display system information

V9 New Features:
- âœ… Unified configuration management
- ğŸ”§ User-configurable parameters
- ğŸ“¦ Dynamic Python version detection (æ”¯æŒæ‰€æœ‰ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬æœªæ¥ç‰ˆæœ¬)
- ğŸš€ No manual 'source .venv/bin/activate' required
- ğŸ” Smart lib directory scanning
- ğŸ“‹ Detailed startup diagnostics
- âš™ï¸ Real-time configuration updates

Current Configuration:
- ğŸ“„ Content Display Limit: {config.content_limits.markdown_display_limit} chars
- ğŸ¯ Word Count Threshold: {config.quality_control.word_count_threshold} words
- â±ï¸ Page Timeout: {config.timing_control.page_timeout_ms}ms
- ğŸ”„ Max Retries: {config.retry_control.max_retries}
- ğŸ‘¤ Show Word Count: {config.user_preferences.show_word_count}
- ğŸ‘¤ Show Detailed Logs: {config.user_preferences.show_detailed_logs}

Configuration Management:
- ğŸ”§ View all settings: configure_crawl_settings("show", "all")
- âš¡ Quick content limit: quick_config_content_limit(5000)
- âš¡ Quick word threshold: quick_config_word_threshold(100)
- ğŸ”„ Reset to defaults: configure_crawl_settings("reset")

System: Ready for web crawling operations with unified configuration management"""

        return status_info
        
    except Exception as e:
        return f"System Status Error: {str(e)}"

# ===== å¯åŠ¨ä¿¡æ¯ =====

def show_v9_welcome():
    """Display V9 startup information"""
    global config
    
    print("Context Scraper V9 MCP Server")
    print("=" * 50)
    print("ğŸ†• V9 New Features:")
    print("   âœ… Unified configuration management")
    print("   ğŸ”§ User-configurable parameters")
    print("   ğŸ“¦ Dynamic Python version detection (æ”¯æŒæœªæ¥ç‰ˆæœ¬)")
    print("   ğŸš€ No manual activation required")
    print("   ğŸ” Smart lib directory scanning")
    print("   ğŸ“‹ Detailed startup diagnostics")
    print("   âš™ï¸ Real-time configuration updates")
    print()
    print("Configuration Status:")
    print(f"   ğŸ“„ Content Limit: {config.content_limits.markdown_display_limit} chars")
    print(f"   ğŸ¯ Word Threshold: {config.quality_control.word_count_threshold} words")
    print(f"   â±ï¸ Page Timeout: {config.timing_control.page_timeout_ms}ms")
    print(f"   ğŸ‘¤ Show Word Count: {config.user_preferences.show_word_count}")
    print()
    print("Python Version Support:")
    print("   ğŸ¯ Current: All existing Python versions")
    print("   ğŸš€ Future-proof: Automatically detects new versions")
    print("   ğŸ”„ Dynamic: Scans .venv/lib/ for python* directories")
    print("   ğŸ“Š Priority: Uses latest available version")
    print()
    print("Main Features:")
    print("   - Smart web crawling and content extraction")
    print("   - Search guide and best practices")
    print("   - Intent analysis and strategy optimization")
    print("   - Experimental Claude analysis feature")
    print("   - Unified configuration management")
    print("=" * 50)
    print("Usage Instructions:")
    print("   - smart_search_guide: Smart search guide")
    print("   - crawl_with_intelligence: Smart web crawling")
    print("   - crawl_stealth: Stealth mode crawling")
    print("   - crawl_with_geolocation: Geolocation spoofing")
    print("   - crawl_with_retry: Retry mode crawling")
    print("   - experimental_claude_analysis: Claude analysis")
    print("   - configure_crawl_settings: Configuration management")
    print()
    print("Configuration Commands:")
    print("   - configure_crawl_settings('show', 'all'): View all settings")
    print("   - quick_config_content_limit(5000): Set content limit")
    print("   - quick_config_word_threshold(100): Set word threshold")
    print()
    print("Search Function:")
    print("   Use smart_search_guide to get search guidance")
    print("   Use existing crawling tools for efficient search")
    print("=" * 50)

if __name__ == "__main__":
    show_v9_welcome()
