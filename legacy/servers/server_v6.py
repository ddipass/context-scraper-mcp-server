# server_v6.py - Context Scraper MCP Server V6
# é‡æ„æ¶æ„ï¼Œæ¶ˆé™¤åè§ï¼Œæ”¯æŒå¤šæœç´¢å¼•æ“

import asyncio
import sys
import os
import json
import time
from typing import Optional, List, Dict, Any
from pathlib import Path

# V6 æ ¸å¿ƒç»„ä»¶
from v6_core.config_manager import get_config, get_user_preferences
from v6_core.intent_analyzer import analyze_user_intent, UserIntent, SearchEngineIntent
from v6_core.search_manager import search_with_intent, search_manager

# ç»§æ‰¿ V5 çš„çˆ¬å–åŠŸèƒ½ (ä¿æŒå…¼å®¹)
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from mcp.server.fastmcp import Context, FastMCP

# Create MCP server
mcp = FastMCP("ContextScraperV6")

# ===== V6 æ ¸å¿ƒåŠŸèƒ½ =====

@mcp.tool()
async def search_with_engine(
    query: str,
    engine: str = None,
    max_results: int = 10,
    ctx: Context = None
) -> str:
    """ğŸ” V6 æ™ºèƒ½æœç´¢ - æ”¯æŒå¤šæœç´¢å¼•æ“ï¼Œå°Šé‡ç”¨æˆ·é€‰æ‹©
    
    å‚æ•°:
    - query: æœç´¢æŸ¥è¯¢è¯
    - engine: æŒ‡å®šæœç´¢å¼•æ“ (google/baidu/bing/yahoo/duckduckgo)
    - max_results: æœ€å¤§ç»“æœæ•°é‡
    
    ç‰¹æ€§:
    - ğŸ¯ ä¸¥æ ¼éµå¾ªç”¨æˆ·æŒ‡å®šçš„æœç´¢å¼•æ“
    - ğŸ”„ æ™ºèƒ½å›é€€æœºåˆ¶
    - ğŸŒ æ”¯æŒå¤šè¯­è¨€å’Œåœ°åŒºåå¥½
    - âš¡ å¹¶å‘æœç´¢ä¼˜åŒ–
    """
    
    try:
        # åˆ†æç”¨æˆ·æ„å›¾
        intent = analyze_user_intent(f"{engine or ''} {query}".strip())
        
        # å¦‚æœç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†æœç´¢å¼•æ“ï¼Œå¼ºåˆ¶ä½¿ç”¨
        if engine:
            intent.search_engine = engine.lower()
            intent.search_engine_intent = SearchEngineIntent.EXPLICIT
        
        # æ‰§è¡Œæœç´¢
        response = await search_with_intent(query, intent, max_results)
        
        if response.success and response.results:
            # æ ¼å¼åŒ–æœç´¢ç»“æœ
            result_text = f"ğŸ” **æœç´¢ç»“æœ** (å¼•æ“: {response.engine})\n"
            result_text += f"ğŸ“Š æŸ¥è¯¢: {response.query}\n"
            result_text += f"â±ï¸ ç”¨æ—¶: {response.search_time:.2f}ç§’\n"
            result_text += f"ğŸ“ˆ ç»“æœæ•°: {len(response.results)}\n\n"
            
            for i, result in enumerate(response.results, 1):
                result_text += f"**{i}. {result.title}**\n"
                result_text += f"ğŸ”— {result.url}\n"
                if result.snippet:
                    result_text += f"ğŸ“ {result.snippet}\n"
                result_text += "\n"
            
            return result_text
        else:
            error_msg = f"âŒ æœç´¢å¤±è´¥\n"
            error_msg += f"ğŸ” æŸ¥è¯¢: {query}\n"
            error_msg += f"ğŸ”§ å¼•æ“: {response.engine}\n"
            if response.error_message:
                error_msg += f"ğŸ’¬ é”™è¯¯: {response.error_message}\n"
            
            return error_msg
            
    except Exception as e:
        return f"âŒ æœç´¢è¿‡ç¨‹å‡ºé”™: {str(e)}"

@mcp.tool()
async def smart_research_v6(
    research_query: str,
    preferred_engine: str = None,
    depth: str = "standard",
    ctx: Context = None
) -> str:
    """ğŸ§  V6 æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ - åŸºäºæœç´¢ç»“æœçš„æ·±åº¦ç ”ç©¶
    
    å‚æ•°:
    - research_query: ç ”ç©¶é—®é¢˜
    - preferred_engine: é¦–é€‰æœç´¢å¼•æ“
    - depth: ç ”ç©¶æ·±åº¦ (quick/standard/deep)
    
    ç‰¹æ€§:
    - ğŸ¯ æ„å›¾é©±åŠ¨çš„ç ”ç©¶ç­–ç•¥
    - ğŸ” å¤šå¼•æ“æœç´¢èšåˆ
    - ğŸ“Š ç»“æ„åŒ–ç ”ç©¶æŠ¥å‘Š
    - ğŸš€ å®æ—¶è¿›åº¦åé¦ˆ
    """
    
    try:
        start_time = time.time()
        
        # åˆ†æç ”ç©¶æ„å›¾
        intent = analyze_user_intent(f"{preferred_engine or ''} {research_query}".strip())
        
        if preferred_engine:
            intent.search_engine = preferred_engine.lower()
            intent.search_engine_intent = SearchEngineIntent.EXPLICIT
        
        # ç”Ÿæˆæœç´¢å…³é”®è¯
        search_keywords = intent.search_keywords or [research_query]
        
        # æ‰§è¡Œå¤šè½®æœç´¢
        all_results = []
        for keyword in search_keywords[:3]:  # é™åˆ¶å…³é”®è¯æ•°é‡
            response = await search_with_intent(keyword, intent, 5)
            if response.success:
                all_results.extend(response.results)
        
        if not all_results:
            return f"âŒ æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ: {research_query}"
        
        # å»é‡å’Œæ’åº
        unique_results = {}
        for result in all_results:
            if result.url not in unique_results:
                unique_results[result.url] = result
        
        sorted_results = sorted(unique_results.values(), key=lambda x: x.rank)
        
        # æ ¹æ®æ·±åº¦å†³å®šçˆ¬å–æ•°é‡
        crawl_count = {"quick": 2, "standard": 5, "deep": 10}.get(depth, 5)
        
        # çˆ¬å–è¯¦ç»†å†…å®¹
        detailed_content = []
        for result in sorted_results[:crawl_count]:
            try:
                browser_config = BrowserConfig(headless=True)
                async with AsyncWebCrawler(config=browser_config) as crawler:
                    crawl_result = await crawler.arun(
                        url=result.url,
                        config=CrawlerRunConfig(
                            cache_mode=CacheMode.BYPASS,
                            word_count_threshold=100
                        )
                    )
                    
                    if crawl_result.success and crawl_result.markdown:
                        detailed_content.append({
                            "title": result.title,
                            "url": result.url,
                            "content": crawl_result.markdown[:2000]  # é™åˆ¶é•¿åº¦
                        })
            except Exception as e:
                continue
        
        # ç”Ÿæˆç ”ç©¶æŠ¥å‘Š
        elapsed_time = time.time() - start_time
        
        report = f"ğŸ“‹ **V6 æ™ºèƒ½ç ”ç©¶æŠ¥å‘Š**\n\n"
        report += f"ğŸ¯ **ç ”ç©¶ä¸»é¢˜**: {research_query}\n"
        report += f"ğŸ” **æœç´¢å¼•æ“**: {intent.search_engine or 'æ™ºèƒ½é€‰æ‹©'}\n"
        report += f"ğŸ“Š **ç ”ç©¶æ·±åº¦**: {depth}\n"
        report += f"â±ï¸ **ç”¨æ—¶**: {elapsed_time:.2f}ç§’\n"
        report += f"ğŸ“ˆ **æ•°æ®æº**: {len(detailed_content)}ä¸ªç½‘ç«™\n\n"
        
        # æ„å›¾åˆ†æç»“æœ
        if intent.search_engine_intent == SearchEngineIntent.EXPLICIT:
            report += f"âœ… **ç”¨æˆ·æŒ‡å®šæœç´¢å¼•æ“**: {intent.search_engine} (å·²ä¸¥æ ¼éµå¾ª)\n\n"
        
        # æœç´¢ç»“æœæ‘˜è¦
        report += f"## ğŸ” æœç´¢ç»“æœæ‘˜è¦\n\n"
        for i, result in enumerate(sorted_results[:10], 1):
            report += f"**{i}. {result.title}**\n"
            report += f"   ğŸ”— {result.url}\n"
            if result.snippet:
                report += f"   ğŸ“ {result.snippet[:200]}...\n"
            report += "\n"
        
        # è¯¦ç»†å†…å®¹åˆ†æ
        if detailed_content:
            report += f"## ğŸ“Š è¯¦ç»†å†…å®¹åˆ†æ\n\n"
            for i, content in enumerate(detailed_content, 1):
                report += f"### {i}. {content['title']}\n\n"
                report += f"ğŸ”— æ¥æº: {content['url']}\n\n"
                report += f"ğŸ“„ å†…å®¹æ‘˜è¦:\n{content['content'][:1000]}...\n\n"
                report += "---\n\n"
        
        return report
        
    except Exception as e:
        return f"âŒ ç ”ç©¶è¿‡ç¨‹å‡ºé”™: {str(e)}"

@mcp.tool()
async def configure_search_engines(
    action: str,
    engine: str = None,
    setting: str = None,
    value: str = None,
    ctx: Context = None
) -> str:
    """âš™ï¸ V6 æœç´¢å¼•æ“é…ç½®ç®¡ç†
    
    å‚æ•°:
    - action: æ“ä½œç±»å‹ (list/enable/disable/set_default/set_priority)
    - engine: æœç´¢å¼•æ“åç§°
    - setting: è®¾ç½®é¡¹
    - value: è®¾ç½®å€¼
    
    ç¤ºä¾‹:
    - list: åˆ—å‡ºæ‰€æœ‰æœç´¢å¼•æ“
    - enable google: å¯ç”¨Googleæœç´¢
    - set_default baidu: è®¾ç½®ç™¾åº¦ä¸ºé»˜è®¤æœç´¢å¼•æ“
    """
    
    try:
        config = get_config()
        
        if action == "list":
            # åˆ—å‡ºæ‰€æœ‰æœç´¢å¼•æ“
            engines_info = search_manager.get_engine_info()
            
            result = "ğŸ”§ **æœç´¢å¼•æ“é…ç½®**\n\n"
            result += f"ğŸ¯ **é»˜è®¤å¼•æ“**: {config.user_preferences.default_search_engine}\n"
            result += f"âœ… **ä¸¥æ ¼éµå¾ªç”¨æˆ·æŒ‡å®š**: {config.user_preferences.respect_explicit_engine}\n"
            result += f"ğŸ”„ **è‡ªåŠ¨å›é€€**: {config.user_preferences.auto_fallback}\n\n"
            
            result += "## ğŸ“‹ å¯ç”¨æœç´¢å¼•æ“\n\n"
            
            for name, info in engines_info.items():
                status = "âœ… å¯ç”¨" if info["enabled"] else "âŒ ç¦ç”¨"
                result += f"**{info['name']}** ({name})\n"
                result += f"   çŠ¶æ€: {status}\n"
                result += f"   ä¼˜å…ˆçº§: {info['priority']}\n"
                result += f"   è¶…æ—¶: {info['timeout']}ç§’\n"
                result += f"   æœ€å¤§ç»“æœ: {info['max_results']}\n\n"
            
            return result
            
        elif action == "enable" and engine:
            config.enable_search_engine(engine)
            return f"âœ… å·²å¯ç”¨æœç´¢å¼•æ“: {engine}"
            
        elif action == "disable" and engine:
            config.disable_search_engine(engine)
            return f"âŒ å·²ç¦ç”¨æœç´¢å¼•æ“: {engine}"
            
        elif action == "set_default" and engine:
            config.update_user_preference("default_search_engine", engine)
            return f"ğŸ¯ å·²è®¾ç½®é»˜è®¤æœç´¢å¼•æ“: {engine}"
            
        elif action == "health_check":
            health_status = await search_manager.health_check_all()
            
            result = "ğŸ¥ **æœç´¢å¼•æ“å¥åº·æ£€æŸ¥**\n\n"
            for engine, is_healthy in health_status.items():
                status = "âœ… æ­£å¸¸" if is_healthy else "âŒ å¼‚å¸¸"
                result += f"**{engine}**: {status}\n"
            
            return result
            
        else:
            return "âŒ æ— æ•ˆçš„é…ç½®æ“ä½œã€‚æ”¯æŒçš„æ“ä½œ: list, enable, disable, set_default, health_check"
            
    except Exception as e:
        return f"âŒ é…ç½®æ“ä½œå¤±è´¥: {str(e)}"

@mcp.tool()
async def analyze_search_intent(
    user_input: str,
    ctx: Context = None
) -> str:
    """ğŸ¯ V6 æ„å›¾åˆ†æ - åˆ†æç”¨æˆ·æœç´¢æ„å›¾å’Œåå¥½
    
    å‚æ•°:
    - user_input: ç”¨æˆ·è¾“å…¥çš„æœç´¢è¯·æ±‚
    
    åŠŸèƒ½:
    - è¯†åˆ«æ˜ç¡®æŒ‡å®šçš„æœç´¢å¼•æ“
    - åˆ†æå†…å®¹ç±»å‹å’Œè¯­è¨€åå¥½
    - æ£€æµ‹ç‰¹æ®Šéœ€æ±‚ (éšèº«ã€åŠ¨æ€å†…å®¹ç­‰)
    - æä¾›æœç´¢å»ºè®®
    """
    
    try:
        # åˆ†æç”¨æˆ·æ„å›¾
        intent = analyze_user_intent(user_input)
        
        # æ ¼å¼åŒ–åˆ†æç»“æœ
        result = "ğŸ¯ **ç”¨æˆ·æ„å›¾åˆ†æ**\n\n"
        result += f"ğŸ“ **åŸå§‹è¾“å…¥**: {user_input}\n\n"
        
        # ä¸»è¦æ„å›¾
        result += f"ğŸª **ä¸»è¦æ„å›¾**: {intent.primary_intent.value}\n"
        result += f"ğŸ“Š **ç½®ä¿¡åº¦**: {intent.confidence:.2f}\n\n"
        
        # æœç´¢å¼•æ“æ„å›¾
        if intent.search_engine:
            engine_type_map = {
                SearchEngineIntent.EXPLICIT: "ğŸ¯ æ˜ç¡®æŒ‡å®š",
                SearchEngineIntent.IMPLICIT: "ğŸ’¡ éšå«åå¥½", 
                SearchEngineIntent.AUTO: "ğŸ¤– è‡ªåŠ¨é€‰æ‹©"
            }
            engine_type = engine_type_map.get(intent.search_engine_intent, "æœªçŸ¥")
            
            result += f"ğŸ” **æœç´¢å¼•æ“**: {intent.search_engine}\n"
            result += f"ğŸ­ **æŒ‡å®šæ–¹å¼**: {engine_type}\n\n"
            
            if intent.search_engine_intent == SearchEngineIntent.EXPLICIT:
                result += "âœ… **V6æ‰¿è¯º**: å°†ä¸¥æ ¼ä½¿ç”¨æ‚¨æŒ‡å®šçš„æœç´¢å¼•æ“\n\n"
        else:
            result += "ğŸ” **æœç´¢å¼•æ“**: å°†æ™ºèƒ½é€‰æ‹©æœ€é€‚åˆçš„å¼•æ“\n\n"
        
        # æœç´¢å…³é”®è¯
        if intent.search_keywords:
            result += f"ğŸ”‘ **æœç´¢å…³é”®è¯**: {', '.join(intent.search_keywords)}\n\n"
        
        # å†…å®¹ç±»å‹å’Œè¯­è¨€åå¥½
        if intent.content_type:
            result += f"ğŸ“„ **å†…å®¹ç±»å‹**: {intent.content_type}\n"
        
        if intent.language_preference:
            result += f"ğŸŒ **è¯­è¨€åå¥½**: {intent.language_preference}\n"
        
        # ç‰¹æ®Šéœ€æ±‚
        special_needs = []
        if intent.stealth_required:
            special_needs.append("ğŸ¥· éšèº«æ¨¡å¼")
        if intent.dynamic_content:
            special_needs.append("âš¡ åŠ¨æ€å†…å®¹")
        if intent.batch_processing:
            special_needs.append("ğŸ“¦ æ‰¹é‡å¤„ç†")
        
        if special_needs:
            result += f"ğŸª **ç‰¹æ®Šéœ€æ±‚**: {', '.join(special_needs)}\n"
        
        result += "\n"
        
        # æ¨èçš„æœç´¢å¼•æ“
        from v6_core.intent_analyzer import intent_analyzer
        recommended_engine = intent_analyzer.get_search_engine_recommendation(intent)
        result += f"ğŸ’¡ **æ¨èæœç´¢å¼•æ“**: {recommended_engine}\n"
        
        # å¦‚æœç”¨æˆ·æ²¡æœ‰æ˜ç¡®æŒ‡å®šï¼Œæä¾›å»ºè®®
        if intent.search_engine_intent != SearchEngineIntent.EXPLICIT:
            result += f"ğŸ’­ **å»ºè®®**: å¦‚éœ€ä½¿ç”¨ç‰¹å®šæœç´¢å¼•æ“ï¼Œè¯·åœ¨æŸ¥è¯¢ä¸­æ˜ç¡®æŒ‡å®šï¼Œå¦‚ 'ç”¨Googleæœç´¢...' æˆ– 'ç™¾åº¦æœç´¢...'\n"
        
        return result
        
    except Exception as e:
        return f"âŒ æ„å›¾åˆ†æå¤±è´¥: {str(e)}"

@mcp.tool()
async def v6_system_status(ctx: Context = None) -> str:
    """ğŸ“Š V6 ç³»ç»ŸçŠ¶æ€ - æŸ¥çœ‹ç³»ç»Ÿè¿è¡ŒçŠ¶æ€å’Œé…ç½®
    """
    
    try:
        config = get_config()
        
        # ç³»ç»ŸåŸºæœ¬ä¿¡æ¯
        result = "ğŸ“Š **Context Scraper V6 ç³»ç»ŸçŠ¶æ€**\n\n"
        result += f"ğŸš€ **ç‰ˆæœ¬**: {config.system_config.version}\n"
        result += f"ğŸ› **è°ƒè¯•æ¨¡å¼**: {config.system_config.debug_mode}\n"
        result += f"ğŸ“ **æ—¥å¿—çº§åˆ«**: {config.system_config.log_level}\n\n"
        
        # æœç´¢å¼•æ“çŠ¶æ€
        available_engines = search_manager.get_available_engines()
        result += f"ğŸ” **å¯ç”¨æœç´¢å¼•æ“**: {len(available_engines)}ä¸ª\n"
        result += f"ğŸ“‹ **å¼•æ“åˆ—è¡¨**: {', '.join(available_engines)}\n"
        result += f"ğŸ¯ **é»˜è®¤å¼•æ“**: {config.user_preferences.default_search_engine}\n\n"
        
        # ç”¨æˆ·åå¥½
        prefs = config.user_preferences
        result += "âš™ï¸ **ç”¨æˆ·åå¥½**\n"
        result += f"   âœ… ä¸¥æ ¼éµå¾ªæŒ‡å®šå¼•æ“: {prefs.respect_explicit_engine}\n"
        result += f"   ğŸ”„ è‡ªåŠ¨å›é€€: {prefs.auto_fallback}\n"
        result += f"   ğŸ‡¨ğŸ‡³ ä¸­æ–‡å†…å®¹å¼•æ“: {prefs.chinese_content_engine}\n"
        result += f"   ğŸ“ å­¦æœ¯å†…å®¹å¼•æ“: {prefs.academic_content_engine}\n"
        result += f"   ğŸ”’ éšç§ä¿æŠ¤å¼•æ“: {prefs.privacy_focused_engine}\n\n"
        
        # æ€§èƒ½é…ç½®
        sys_config = config.system_config
        result += "âš¡ **æ€§èƒ½é…ç½®**\n"
        result += f"   ğŸ”„ æœ€å¤§å¹¶å‘è¯·æ±‚: {sys_config.max_concurrent_requests}\n"
        result += f"   â±ï¸ è¯·æ±‚è¶…æ—¶: {sys_config.request_timeout}ç§’\n"
        result += f"   ğŸ” é‡è¯•æ¬¡æ•°: {sys_config.retry_attempts}\n"
        result += f"   ğŸ’¾ ç¼“å­˜å¯ç”¨: {sys_config.cache_enabled}\n\n"
        
        # å¥åº·æ£€æŸ¥
        try:
            health_status = await search_manager.health_check_all()
            healthy_count = sum(1 for status in health_status.values() if status)
            total_count = len(health_status)
            
            result += f"ğŸ¥ **å¥åº·çŠ¶æ€**: {healthy_count}/{total_count} å¼•æ“æ­£å¸¸\n"
            
            for engine, is_healthy in health_status.items():
                status_icon = "âœ…" if is_healthy else "âŒ"
                result += f"   {status_icon} {engine}\n"
        
        except Exception as e:
            result += f"ğŸ¥ **å¥åº·æ£€æŸ¥**: æ£€æŸ¥å¤±è´¥ - {str(e)}\n"
        
        result += "\nğŸ‰ **V6 æ ¸å¿ƒç‰¹æ€§**\n"
        result += "   ğŸ¯ ç”¨æˆ·æ„å›¾è‡³ä¸Š - ä¸¥æ ¼éµå¾ªæ˜ç¡®æŒ‡å®š\n"
        result += "   ğŸ” å¤šæœç´¢å¼•æ“æ”¯æŒ - Google/ç™¾åº¦/Bingç­‰\n"
        result += "   ğŸ§  æ— åè§æ„å›¾åˆ†æ - æ¶ˆé™¤ç³»ç»Ÿå›ºåŒ–åå¥½\n"
        result += "   âš™ï¸ ç»Ÿä¸€é…ç½®ç®¡ç† - ä¸€ç«™å¼è®¾ç½®ä¸­å¿ƒ\n"
        result += "   ğŸ”„ æ™ºèƒ½å›é€€æœºåˆ¶ - ç¡®ä¿æœç´¢æˆåŠŸç‡\n"
        
        return result
        
    except Exception as e:
        return f"âŒ ç³»ç»ŸçŠ¶æ€æŸ¥è¯¢å¤±è´¥: {str(e)}"

# ===== å…¼å®¹æ€§å·¥å…· - ç»§æ‰¿V5åŠŸèƒ½ =====

@mcp.tool()
async def crawl_with_v6_intelligence(
    url: str,
    use_smart_analysis: bool = True,
    ctx: Context = None
) -> str:
    """ğŸ•·ï¸ V6 æ™ºèƒ½çˆ¬å– - ç»“åˆæ„å›¾åˆ†æçš„ç½‘é¡µçˆ¬å–
    
    å‚æ•°:
    - url: ç›®æ ‡ç½‘å€
    - use_smart_analysis: æ˜¯å¦ä½¿ç”¨æ™ºèƒ½åˆ†æ
    
    ç‰¹æ€§:
    - ğŸ§  åŸºäºURLçš„æ™ºèƒ½åˆ†æ
    - ğŸ¯ è‡ªé€‚åº”çˆ¬å–ç­–ç•¥
    - ğŸ“Š ç»“æ„åŒ–å†…å®¹æå–
    """
    
    try:
        # åˆ†æURLæ„å›¾
        if use_smart_analysis:
            intent = analyze_user_intent(f"çˆ¬å– {url}")
            
            # æ ¹æ®æ„å›¾è°ƒæ•´çˆ¬å–ç­–ç•¥
            browser_config = BrowserConfig(
                headless=True,
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            )
            
            crawl_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                word_count_threshold=50,
                delay_before_return_html=2 if intent.dynamic_content else 0
            )
        else:
            browser_config = BrowserConfig(headless=True)
            crawl_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
        
        # æ‰§è¡Œçˆ¬å–
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=crawl_config)
            
            if result.success:
                response = f"ğŸ•·ï¸ **V6 æ™ºèƒ½çˆ¬å–ç»“æœ**\n\n"
                response += f"ğŸ”— **URL**: {url}\n"
                response += f"ğŸ“„ **æ ‡é¢˜**: {result.metadata.get('title', 'æœªçŸ¥')}\n"
                response += f"ğŸ“Š **å­—æ•°**: {len(result.markdown.split()) if result.markdown else 0}\n"
                
                if use_smart_analysis:
                    response += f"ğŸ§  **æ™ºèƒ½åˆ†æ**: å·²å¯ç”¨\n"
                
                response += f"\nğŸ“ **å†…å®¹**:\n\n{result.markdown[:3000]}..."
                
                return response
            else:
                return f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}"
                
    except Exception as e:
        return f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {str(e)}"

# ===== å¯åŠ¨ä¿¡æ¯ =====

def show_v6_welcome():
    """æ˜¾ç¤ºV6æ¬¢è¿ä¿¡æ¯"""
    print("ğŸš€ Context Scraper V6 å¯åŠ¨æˆåŠŸ!")
    print("=" * 50)
    print("ğŸ¯ æ ¸å¿ƒç‰¹æ€§:")
    print("   âœ… ç”¨æˆ·æ„å›¾è‡³ä¸Š - ä¸¥æ ¼éµå¾ªæ˜ç¡®æŒ‡å®š")
    print("   ğŸ” å¤šæœç´¢å¼•æ“æ”¯æŒ - Google/ç™¾åº¦/Bing/Yahoo/DuckDuckGo")
    print("   ğŸ§  æ— åè§æ„å›¾åˆ†æ - æ¶ˆé™¤ç³»ç»Ÿå›ºåŒ–åå¥½")
    print("   âš™ï¸ ç»Ÿä¸€é…ç½®ç®¡ç† - ä¸€ç«™å¼è®¾ç½®ä¸­å¿ƒ")
    print("   ğŸ”„ æ™ºèƒ½å›é€€æœºåˆ¶ - ç¡®ä¿æœç´¢æˆåŠŸç‡")
    print("=" * 50)
    print("ğŸ’¡ ä½¿ç”¨æç¤º:")
    print("   - æ˜ç¡®æŒ‡å®šæœç´¢å¼•æ“: 'ç”¨Googleæœç´¢AIæ–°é—»'")
    print("   - æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€: v6_system_status")
    print("   - é…ç½®æœç´¢å¼•æ“: configure_search_engines")
    print("   - åˆ†ææœç´¢æ„å›¾: analyze_search_intent")
    print("=" * 50)

if __name__ == "__main__":
    show_v6_welcome()
