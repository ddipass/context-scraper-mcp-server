# server_v5_core.py - Context Scraper V5 æ ¸å¿ƒæ¶æ„
# åŸºäºV4ï¼Œå®ç°åˆ†å±‚æ‰§è¡Œå¼•æ“å’Œå®æ—¶è¿›åº¦åé¦ˆ

import asyncio
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum

class ResearchMode(Enum):
    """ç ”ç©¶æ¨¡å¼æšä¸¾"""
    QUICK = "quick"      # 3-8ç§’
    STANDARD = "standard"  # 15-25ç§’  
    DEEP = "deep"        # 30-60ç§’
    AUTO = "auto"        # è‡ªåŠ¨é€‰æ‹©

class ExecutionStage(Enum):
    """æ‰§è¡Œé˜¶æ®µæšä¸¾"""
    INIT = "åˆå§‹åŒ–"
    ANALYSIS = "æ„å›¾åˆ†æ" 
    LAYER1 = "å¿«é€Ÿçˆ¬å–"
    LAYER2 = "æ™ºèƒ½åˆ†æ"
    LAYER3 = "æ·±åº¦æŒ–æ˜"
    REPORT = "ç”ŸæˆæŠ¥å‘Š"
    COMPLETE = "å®Œæˆ"

@dataclass
class ProgressInfo:
    """è¿›åº¦ä¿¡æ¯"""
    stage: ExecutionStage
    progress: float  # 0-100
    message: str
    eta_seconds: Optional[int] = None
    start_time: float = 0
    
    def __post_init__(self):
        if self.start_time == 0:
            self.start_time = time.time()

@dataclass
class ResearchContext:
    """ç ”ç©¶ä¸Šä¸‹æ–‡"""
    query: str
    websites: List[str]
    mode: ResearchMode
    keywords: List[str]
    estimated_time: int  # ç§’
    needs_deep_crawl: bool = False
    competitive_analysis: bool = False

class V5ProgressTracker:
    """V5è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.current_stage = ExecutionStage.INIT
        self.start_time = time.time()
        self.stage_times = {
            ExecutionStage.INIT: 1,
            ExecutionStage.ANALYSIS: 2,
            ExecutionStage.LAYER1: 0,  # åŠ¨æ€è®¡ç®—
            ExecutionStage.LAYER2: 0,  # åŠ¨æ€è®¡ç®—
            ExecutionStage.LAYER3: 0,  # åŠ¨æ€è®¡ç®—
            ExecutionStage.REPORT: 3,
            ExecutionStage.COMPLETE: 0
        }
        self.progress_callback = None
    
    def set_mode_timing(self, mode: ResearchMode, num_websites: int):
        """æ ¹æ®æ¨¡å¼è®¾ç½®æ—¶é—´ä¼°ç®—"""
        if mode == ResearchMode.QUICK:
            self.stage_times[ExecutionStage.LAYER1] = min(5, num_websites * 2)
            self.stage_times[ExecutionStage.LAYER2] = 8
            self.stage_times[ExecutionStage.LAYER3] = 0
        elif mode == ResearchMode.STANDARD:
            self.stage_times[ExecutionStage.LAYER1] = min(10, num_websites * 3)
            self.stage_times[ExecutionStage.LAYER2] = 15
            self.stage_times[ExecutionStage.LAYER3] = 0
        elif mode == ResearchMode.DEEP:
            self.stage_times[ExecutionStage.LAYER1] = min(15, num_websites * 4)
            self.stage_times[ExecutionStage.LAYER2] = 20
            self.stage_times[ExecutionStage.LAYER3] = 25
    
    def get_total_estimated_time(self) -> int:
        """è·å–æ€»é¢„ä¼°æ—¶é—´"""
        return sum(self.stage_times.values())
    
    def update_progress(self, stage: ExecutionStage, progress: float, message: str):
        """æ›´æ–°è¿›åº¦"""
        self.current_stage = stage
        
        # è®¡ç®—æ€»ä½“è¿›åº¦
        stage_weights = {
            ExecutionStage.INIT: 5,
            ExecutionStage.ANALYSIS: 10,
            ExecutionStage.LAYER1: 30,
            ExecutionStage.LAYER2: 40,
            ExecutionStage.LAYER3: 10,
            ExecutionStage.REPORT: 5
        }
        
        completed_weight = 0
        for s in ExecutionStage:
            if s.value < stage.value:
                completed_weight += stage_weights.get(s, 0)
        
        current_stage_weight = stage_weights.get(stage, 0)
        total_progress = completed_weight + (current_stage_weight * progress / 100)
        
        # è®¡ç®—ETA
        elapsed = time.time() - self.start_time
        if total_progress > 0:
            eta = int((elapsed / total_progress * 100) - elapsed)
        else:
            eta = self.get_total_estimated_time()
        
        progress_info = ProgressInfo(
            stage=stage,
            progress=total_progress,
            message=message,
            eta_seconds=max(0, eta),
            start_time=self.start_time
        )
        
        if self.progress_callback:
            self.progress_callback(progress_info)
        
        return progress_info

class V5IntentAnalyzer:
    """V5å¢å¼ºæ„å›¾åˆ†æå™¨"""
    
    @staticmethod
    def analyze_research_intent(query: str, websites: List[str]) -> ResearchContext:
        """åˆ†æç ”ç©¶æ„å›¾å¹¶è¿”å›ä¸Šä¸‹æ–‡"""
        query_lower = query.lower()
        
        # æå–å…³é”®è¯
        keywords = [word for word in query.split() if len(word) > 2][:8]
        
        # æ£€æµ‹ç ”ç©¶æ¨¡å¼
        mode = ResearchMode.STANDARD
        needs_deep = False
        competitive = False
        
        # å¿«é€Ÿæ¨¡å¼å…³é”®è¯
        if any(word in query_lower for word in ["å¿«é€Ÿ", "ç®€å•", "æ¦‚è§ˆ", "çœ‹çœ‹", "äº†è§£", "ç®€ä»‹"]):
            mode = ResearchMode.QUICK
            
        # æ·±åº¦æ¨¡å¼å…³é”®è¯  
        elif any(word in query_lower for word in ["æ·±å…¥", "è¯¦ç»†", "å…¨é¢", "å®Œæ•´", "æ·±åº¦", "å½»åº•"]):
            mode = ResearchMode.DEEP
            needs_deep = True
            
        # ç«äº‰åˆ†æå…³é”®è¯
        elif any(word in query_lower for word in ["å¯¹æ¯”", "ç«äº‰", "vs", "æ¯”è¾ƒ", "ç«å“", "å¯¹æ‰‹"]):
            competitive = True
            if len(websites) > 1:
                mode = ResearchMode.STANDARD
        
        # æ ¹æ®ç½‘ç«™æ•°é‡è°ƒæ•´
        if len(websites) > 3:
            mode = ResearchMode.STANDARD if mode == ResearchMode.QUICK else mode
        
        # ä¼°ç®—æ—¶é—´
        time_estimates = {
            ResearchMode.QUICK: 8,
            ResearchMode.STANDARD: 20,
            ResearchMode.DEEP: 45
        }
        
        estimated_time = time_estimates[mode]
        if len(websites) > 1:
            estimated_time += len(websites) * 3
        
        return ResearchContext(
            query=query,
            websites=websites,
            mode=mode,
            keywords=keywords,
            estimated_time=estimated_time,
            needs_deep_crawl=needs_deep,
            competitive_analysis=competitive
        )

class V5LayeredEngine:
    """V5åˆ†å±‚æ‰§è¡Œå¼•æ“"""
    
    def __init__(self):
        self.progress_tracker = V5ProgressTracker()
        self.intent_analyzer = V5IntentAnalyzer()
    
    async def execute_research(
        self, 
        query: str, 
        websites: str,
        mode: str = "auto",
        progress_callback=None
    ) -> Dict[str, Any]:
        """æ‰§è¡Œåˆ†å±‚ç ”ç©¶"""
        
        # è®¾ç½®è¿›åº¦å›è°ƒ
        self.progress_tracker.progress_callback = progress_callback
        
        try:
            # é˜¶æ®µ0: åˆå§‹åŒ–
            self.progress_tracker.update_progress(
                ExecutionStage.INIT, 50, "æ­£åœ¨åˆå§‹åŒ–ç ”ç©¶å¼•æ“..."
            )
            
            # è§£æç½‘ç«™åˆ—è¡¨
            website_list = [url.strip() for url in websites.split(',') if url.strip()]
            if not website_list:
                raise ValueError("è¯·æä¾›è‡³å°‘ä¸€ä¸ªç›®æ ‡ç½‘ç«™")
            
            self.progress_tracker.update_progress(
                ExecutionStage.INIT, 100, f"å·²è¯†åˆ« {len(website_list)} ä¸ªç›®æ ‡ç½‘ç«™"
            )
            
            # é˜¶æ®µ1: æ„å›¾åˆ†æ
            self.progress_tracker.update_progress(
                ExecutionStage.ANALYSIS, 20, "æ­£åœ¨åˆ†æç ”ç©¶æ„å›¾..."
            )
            
            context = self.intent_analyzer.analyze_research_intent(query, website_list)
            
            # å¦‚æœæŒ‡å®šäº†æ¨¡å¼ï¼Œè¦†ç›–è‡ªåŠ¨åˆ†æç»“æœ
            if mode != "auto":
                context.mode = ResearchMode(mode)
            
            # è®¾ç½®æ—¶é—´ä¼°ç®—
            self.progress_tracker.set_mode_timing(context.mode, len(website_list))
            
            self.progress_tracker.update_progress(
                ExecutionStage.ANALYSIS, 100, 
                f"å·²é€‰æ‹© {context.mode.value} æ¨¡å¼ï¼Œé¢„è®¡ {context.estimated_time} ç§’"
            )
            
            # æ‰§è¡Œåˆ†å±‚å¤„ç†
            result = {
                "context": context,
                "layer1_result": None,
                "layer2_result": None, 
                "layer3_result": None,
                "final_report": None
            }
            
            # Layer 1: å¿«é€Ÿçˆ¬å–
            result["layer1_result"] = await self._execute_layer1(context)
            
            # Layer 2: æ™ºèƒ½åˆ†æ 
            result["layer2_result"] = await self._execute_layer2(context, result["layer1_result"])
            
            # Layer 3: æ·±åº¦æŒ–æ˜ (å¦‚æœéœ€è¦)
            if context.needs_deep_crawl or context.mode == ResearchMode.DEEP:
                result["layer3_result"] = await self._execute_layer3(context, result)
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            result["final_report"] = await self._generate_final_report(context, result)
            
            self.progress_tracker.update_progress(
                ExecutionStage.COMPLETE, 100, "ç ”ç©¶å®Œæˆï¼"
            )
            
            return result
            
        except Exception as e:
            error_msg = f"ç ”ç©¶æ‰§è¡Œå¤±è´¥: {str(e)}"
            self.progress_tracker.update_progress(
                ExecutionStage.COMPLETE, 0, error_msg
            )
            raise
    
    async def _execute_layer1(self, context: ResearchContext) -> Dict[str, Any]:
        """Layer 1: å¿«é€Ÿçˆ¬å–"""
        self.progress_tracker.update_progress(
            ExecutionStage.LAYER1, 10, "å¼€å§‹å¿«é€Ÿçˆ¬å–..."
        )
        
        # è¿™é‡Œä¼šè°ƒç”¨V4çš„çˆ¬å–åŠŸèƒ½
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿçˆ¬å–æ—¶é—´
        
        self.progress_tracker.update_progress(
            ExecutionStage.LAYER1, 100, f"å·²çˆ¬å– {len(context.websites)} ä¸ªç½‘ç«™"
        )
        
        return {
            "websites_crawled": len(context.websites),
            "content_length": 5000,  # æ¨¡æ‹Ÿ
            "success_rate": 0.9
        }
    
    async def _execute_layer2(self, context: ResearchContext, layer1_result: Dict) -> Dict[str, Any]:
        """Layer 2: æ™ºèƒ½åˆ†æ"""
        self.progress_tracker.update_progress(
            ExecutionStage.LAYER2, 20, "æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†æ..."
        )
        
        # è¿™é‡Œä¼šè°ƒç”¨Claudeåˆ†æ
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿåˆ†ææ—¶é—´
        
        self.progress_tracker.update_progress(
            ExecutionStage.LAYER2, 100, "æ™ºèƒ½åˆ†æå®Œæˆ"
        )
        
        return {
            "analysis_type": context.mode.value,
            "key_insights": ["æ´å¯Ÿ1", "æ´å¯Ÿ2", "æ´å¯Ÿ3"],
            "confidence": 0.85
        }
    
    async def _execute_layer3(self, context: ResearchContext, previous_results: Dict) -> Dict[str, Any]:
        """Layer 3: æ·±åº¦æŒ–æ˜"""
        self.progress_tracker.update_progress(
            ExecutionStage.LAYER3, 30, "å¼€å§‹æ·±åº¦æŒ–æ˜..."
        )
        
        # æ·±åº¦çˆ¬å–é€»è¾‘
        await asyncio.sleep(3)  # æ¨¡æ‹Ÿæ·±åº¦å¤„ç†
        
        self.progress_tracker.update_progress(
            ExecutionStage.LAYER3, 100, "æ·±åº¦æŒ–æ˜å®Œæˆ"
        )
        
        return {
            "deep_pages_found": 8,
            "additional_insights": ["æ·±åº¦æ´å¯Ÿ1", "æ·±åº¦æ´å¯Ÿ2"],
            "relevance_score": 0.92
        }
    
    async def _generate_final_report(self, context: ResearchContext, results: Dict) -> str:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        self.progress_tracker.update_progress(
            ExecutionStage.REPORT, 50, "æ­£åœ¨ç”Ÿæˆç ”ç©¶æŠ¥å‘Š..."
        )
        
        await asyncio.sleep(1)  # æ¨¡æ‹ŸæŠ¥å‘Šç”Ÿæˆ
        
        self.progress_tracker.update_progress(
            ExecutionStage.REPORT, 100, "æŠ¥å‘Šç”Ÿæˆå®Œæˆ"
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
# ğŸ”¬ V5æ™ºèƒ½ç ”ç©¶æŠ¥å‘Š: {context.query}

## ğŸ“Š æ‰§è¡Œæ¦‚è§ˆ
- **ç ”ç©¶æ¨¡å¼**: {context.mode.value}
- **ç›®æ ‡ç½‘ç«™**: {len(context.websites)} ä¸ª
- **æ‰§è¡Œæ—¶é—´**: {int(time.time() - self.progress_tracker.start_time)} ç§’
- **å…³é”®è¯**: {', '.join(context.keywords[:5])}

## ğŸ¯ æ ¸å¿ƒå‘ç°
[è¿™é‡Œå°†åŒ…å«Layer 2çš„æ™ºèƒ½åˆ†æç»“æœ]

## ğŸ“ˆ è¯¦ç»†åˆ†æ  
[è¿™é‡Œå°†åŒ…å«å…·ä½“çš„åˆ†æå†…å®¹]

## ğŸ” æ·±åº¦æ´å¯Ÿ
{f"[æ·±åº¦æŒ–æ˜å‘ç° {results.get('layer3_result', {}).get('deep_pages_found', 0)} ä¸ªç›¸å…³é¡µé¢]" if results.get('layer3_result') else "[æœªå¯ç”¨æ·±åº¦æŒ–æ˜]"}

## ğŸ“‹ æ•°æ®æ¥æº
{chr(10).join([f"- {url}" for url in context.websites])}

---
*ç”±Context Scraper V5åˆ†å±‚å¼•æ“ç”Ÿæˆ*
"""
        
        return report

# å¯¼å‡ºæ ¸å¿ƒç±»
__all__ = [
    'V5LayeredEngine',
    'V5ProgressTracker', 
    'V5IntentAnalyzer',
    'ResearchMode',
    'ExecutionStage',
    'ResearchContext',
    'ProgressInfo'
]
