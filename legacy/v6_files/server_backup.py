# server_v6.py - Context Scraper MCP Server V6
# é‡æ„æ¶æ„ï¼Œæ¶ˆé™¤åè§ï¼Œæ”¯æŒå¤šæœç´¢å¼•æ“

import asyncio
import sys
import os
import json
import time
from typing import Optional, List, Dict, Any
from pathlib import Path

# V6 æ ¸å¿ƒç»„ä»¶ (ç®€åŒ–ç‰ˆ)
from v6_core.intent_analyzer import analyze_user_intent, UserIntent, SearchEngineIntent, IntentType

# ç»§æ‰¿ V5 çš„çˆ¬å–åŠŸèƒ½ (ä¿æŒå…¼å®¹)
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from mcp.server.fastmcp import Context, FastMCP

# Create MCP server
mcp = FastMCP("ContextScraperV6")

# ===== V6 æ ¸å¿ƒåŠŸèƒ½ =====

@mcp.prompt()
def smart_search_guide(search_query: str = "ä½ æƒ³æœç´¢ä»€ä¹ˆï¼Ÿ") -> str:
    """æ™ºèƒ½æœç´¢æŒ‡å— - AIé©±åŠ¨çš„æœç´¢å·¥å…·é€‰æ‹©åŠ©æ‰‹
    
    æ ¹æ®æœç´¢å†…å®¹æ™ºèƒ½æ¨èæœ€é€‚åˆçš„çˆ¬å–å·¥å…·ï¼Œ
    æä¾›å†³ç­–æ ‘å’Œå…·ä½“çš„æ‰§è¡Œå»ºè®®ã€‚
    """
    
    # URLç¼–ç å¤„ç†
    import urllib.parse
    encoded_query = urllib.parse.quote_plus(search_query)
    
    # æ„å»ºå„æœç´¢å¼•æ“URL
    google_url = f"https://www.google.com/search?q={encoded_query}"
    baidu_url = f"https://www.baidu.com/s?wd={encoded_query}"
    bing_url = f"https://www.bing.com/search?q={encoded_query}"
    duckduckgo_url = f"https://duckduckgo.com/?q={encoded_query}"
    
    # æ™ºèƒ½åˆ†ææœç´¢å†…å®¹
    query_lower = search_query.lower()
    
    # æ£€æµ‹è¯­è¨€å’Œå†…å®¹ç±»å‹
    has_chinese = any('\u4e00' <= char <= '\u9fff' for char in search_query)
    is_technical = any(word in query_lower for word in ['api', 'code', 'programming', 'ç¼–ç¨‹', 'æŠ€æœ¯', 'github', 'stackoverflow'])
    is_academic = any(word in query_lower for word in ['research', 'paper', 'study', 'ç ”ç©¶', 'è®ºæ–‡', 'å­¦æœ¯'])
    is_news = any(word in query_lower for word in ['news', 'latest', 'æ–°é—»', 'æœ€æ–°'])
    is_sensitive = any(word in query_lower for word in ['privacy', 'anonymous', 'éšç§', 'åŒ¿å'])
    
    # æ™ºèƒ½æ¨è
    if has_chinese and not is_technical:
        primary_recommendation = "ç™¾åº¦æœç´¢"
        primary_url = baidu_url
        primary_reason = "æ£€æµ‹åˆ°ä¸­æ–‡å†…å®¹ï¼Œç™¾åº¦å¯¹ä¸­æ–‡æœç´¢ä¼˜åŒ–æ›´å¥½"
    elif is_technical or is_academic:
        primary_recommendation = "Googleæœç´¢"
        primary_url = google_url
        primary_reason = "æ£€æµ‹åˆ°æŠ€æœ¯/å­¦æœ¯å†…å®¹ï¼ŒGoogleåœ¨è¿™æ–¹é¢èµ„æºæ›´ä¸°å¯Œ"
    elif is_sensitive:
        primary_recommendation = "DuckDuckGoæœç´¢"
        primary_url = duckduckgo_url
        primary_reason = "æ£€æµ‹åˆ°éšç§æ•æ„Ÿå†…å®¹ï¼ŒDuckDuckGoä¸è·Ÿè¸ªç”¨æˆ·"
    else:
        primary_recommendation = "Googleæœç´¢"
        primary_url = google_url
        primary_reason = "é€šç”¨æœç´¢ï¼ŒGoogleè¦†ç›–é¢æœ€å¹¿"
    
    return f"""# ğŸ” AIæ™ºèƒ½æœç´¢åŠ©æ‰‹

## ğŸ¯ æœç´¢åˆ†æ
**æŸ¥è¯¢å†…å®¹**: {search_query}
**å†…å®¹ç‰¹å¾**: {'ä¸­æ–‡' if has_chinese else 'è‹±æ–‡'} | {'æŠ€æœ¯' if is_technical else ''} {'å­¦æœ¯' if is_academic else ''} {'æ–°é—»' if is_news else ''} {'éšç§' if is_sensitive else ''}

## ğŸš€ AIæ¨èæ–¹æ¡ˆ (ä¼˜å…ˆä½¿ç”¨)

### â­ æ¨è: {primary_recommendation}
**æ¨èç†ç”±**: {primary_reason}

**ç«‹å³æ‰§è¡Œ**:
```
crawl_with_intelligence(
    url="{primary_url}",
    use_smart_analysis=True
)
```

## ğŸ› ï¸ å…¶ä»–å¯é€‰æ–¹æ¡ˆ

### ğŸ“‹ åŸºç¡€æœç´¢é€‰é¡¹
| å¼•æ“ | é€‚ç”¨åœºæ™¯ | æ‰§è¡Œå‘½ä»¤ |
|------|----------|----------|
| ğŸš€ Google | æŠ€æœ¯ã€å­¦æœ¯ã€å›½é™…å†…å®¹ | `crawl_with_intelligence("{google_url}", True)` |
| ğŸ‡¨ğŸ‡³ ç™¾åº¦ | ä¸­æ–‡å†…å®¹ã€æœ¬åœŸä¿¡æ¯ | `crawl_with_intelligence("{baidu_url}", True)` |
| ğŸŒ Bing | å¾®è½¯ç”Ÿæ€ã€å•†ä¸šå†…å®¹ | `crawl_with_intelligence("{bing_url}", True)` |
| ğŸ”’ DuckDuckGo | éšç§ä¿æŠ¤ã€åŒ¿åæœç´¢ | `crawl_with_intelligence("{duckduckgo_url}", True)` |

### ğŸ›¡ï¸ åçˆ¬è™«é€‰é¡¹ (é‡åˆ°é˜»æ‹¦æ—¶ä½¿ç”¨)
| å·¥å…· | é€‚ç”¨åœºæ™¯ | æ‰§è¡Œå‘½ä»¤ |
|------|----------|----------|
| ğŸ¥· éšèº«æ¨¡å¼ | åŸºç¡€åçˆ¬è™«æ£€æµ‹ | `crawl_stealth("{primary_url}")` |
| ğŸŒ åœ°ç†ä¼ªè£… | åœ°åŒºå†…å®¹é™åˆ¶ | `crawl_with_geolocation("{primary_url}", "random")` |
| ğŸ”„ é‡è¯•æ¨¡å¼ | ç½‘ç«™ä¸ç¨³å®š | `crawl_with_retry("{primary_url}", 3)` |

## ğŸ¯ å†³ç­–æµç¨‹

```
å¼€å§‹æœç´¢
    â†“
ä½¿ç”¨AIæ¨èæ–¹æ¡ˆ
    â†“
æˆåŠŸ? â†’ æ˜¯ â†’ å®Œæˆ âœ…
    â†“
    å¦
    â†“
å°è¯•éšèº«æ¨¡å¼
    â†“
æˆåŠŸ? â†’ æ˜¯ â†’ å®Œæˆ âœ…
    â†“
    å¦
    â†“
å°è¯•åœ°ç†ä¼ªè£…
    â†“
æˆåŠŸ? â†’ æ˜¯ â†’ å®Œæˆ âœ…
    â†“
    å¦
    â†“
ä½¿ç”¨é‡è¯•æ¨¡å¼ â†’ å®Œæˆ âœ…
```

## ğŸ’¡ å¿«é€Ÿå¼€å§‹

1. **ç›´æ¥ä½¿ç”¨**: å¤åˆ¶ä¸Šé¢çš„AIæ¨èå‘½ä»¤
2. **é‡åˆ°é—®é¢˜**: æŒ‰å†³ç­–æµç¨‹é€æ­¥å°è¯•
3. **å¯¹æ¯”ç»“æœ**: ä½¿ç”¨ä¸åŒå¼•æ“å¯¹æ¯”

## ğŸ”„ é«˜çº§ç”¨æ³•

### å¤šå¼•æ“å¯¹æ¯”
```bash
# å¯¹æ¯”Googleå’Œç™¾åº¦ç»“æœ
crawl_with_intelligence("{google_url}", True)
crawl_with_intelligence("{baidu_url}", True)
```

### ç»„åˆä½¿ç”¨ (ä¸¥æ ¼é˜²æŠ¤ç½‘ç«™)
```bash
# å…ˆéšèº«ï¼Œå†é‡è¯•
crawl_with_retry("{primary_url}", 3)  # è‡ªåŠ¨åŒ…å«éšèº«æ¨¡å¼
```

**ğŸ‰ å¼€å§‹æœç´¢å§ï¼ä¼˜å…ˆä½¿ç”¨ä¸Šé¢çš„AIæ¨èæ–¹æ¡ˆã€‚**









@mcp.tool()
async def v6_system_status(ctx: Context = None) -> str:
    """ç³»ç»ŸçŠ¶æ€æŸ¥çœ‹ - æ˜¾ç¤ºV6ç³»ç»Ÿè¿è¡ŒçŠ¶æ€
    
    åŠŸèƒ½:
    - æ˜¾ç¤ºç³»ç»Ÿç‰ˆæœ¬å’ŒåŸºæœ¬é…ç½®
    - æ˜¾ç¤ºçˆ¬å–åŠŸèƒ½çŠ¶æ€
    - æ˜¾ç¤ºV6æ ¸å¿ƒç‰¹æ€§è¯´æ˜
    """
    
    try:
        result = "ğŸ“Š **Context Scraper V6 ç³»ç»ŸçŠ¶æ€**\n\n"
        result += f"ğŸš€ **ç‰ˆæœ¬**: 6.0.0\n"
        result += f"ğŸ” **ä¸»è¦åŠŸèƒ½**: æ™ºèƒ½ç½‘é¡µçˆ¬å–\n"
        result += f"ğŸ§  **æ™ºèƒ½åˆ†æ**: å·²å¯ç”¨\n"
        result += f"ğŸ›¡ï¸ **éšèº«ä¿æŠ¤**: å†…ç½®æ”¯æŒ\n\n"
        
        result += "ğŸ‰ **V6 æ ¸å¿ƒç‰¹æ€§**\n"
        result += "   ğŸ•·ï¸ æ™ºèƒ½ç½‘é¡µçˆ¬å– - è‡ªåŠ¨ä¼˜åŒ–ç­–ç•¥\n"
        result += "   ğŸ” æœç´¢æŒ‡å— - åˆ©ç”¨ç°æœ‰å·¥å…·å®ç°æœç´¢\n"
        result += "   ğŸ§  æ„å›¾åˆ†æ - æ™ºèƒ½è°ƒæ•´çˆ¬å–å‚æ•°\n"
        result += "   ğŸ›¡ï¸ åæ£€æµ‹ä¿æŠ¤ - å†…ç½®éšèº«æœºåˆ¶\n"
        result += "   ğŸ§ª Claudeåˆ†æ - å®éªŒæ€§AIåˆ†æåŠŸèƒ½\n\n"
        
        result += "ğŸ’¡ **å¯ç”¨å·¥å…·**\n"
        result += "   - smart_search_guide: æ™ºèƒ½æœç´¢æŒ‡å—\n"
        result += "   - crawl_with_intelligence: æ™ºèƒ½çˆ¬å–\n"
        result += "   - experimental_claude_analysis: Claudeåˆ†æ\n"
        result += "   - v6_system_status: ç³»ç»ŸçŠ¶æ€æŸ¥çœ‹\n"
        
        return result
        
    except Exception as e:
        return f"âŒ ç³»ç»ŸçŠ¶æ€æŸ¥è¯¢å¤±è´¥: {str(e)}"

# ===== å®éªŒæ€§åŠŸèƒ½ - Claude 3.7 æµ‹è¯• =====

@mcp.tool()
async def experimental_claude_analysis(
    content: str,
    analysis_type: str = "general",
    enable_claude: bool = False,
    ctx: Context = None
) -> str:
    """å®éªŒæ€§Claudeåˆ†æ - ä½¿ç”¨Claude 3.7è¿›è¡Œå†…å®¹åˆ†æ(å®éªŒåŠŸèƒ½)
    
    å‚æ•°:
    - content: è¦åˆ†æçš„å†…å®¹
    - analysis_type: åˆ†æç±»å‹ (general/technical/academic/business)
    - enable_claude: å¿…é¡»æ˜ç¡®è®¾ç½®ä¸ºTrueæ‰ä¼šè°ƒç”¨Claude API
    
    è­¦å‘Š:
    - è¿™æ˜¯å®éªŒæ€§åŠŸèƒ½ï¼Œéœ€è¦é…ç½®Claude APIå¯†é’¥
    - ä»…åœ¨enable_claude=Trueæ—¶æ‰ä¼šå®é™…è°ƒç”¨Claude API
    - é»˜è®¤æƒ…å†µä¸‹ä¸ä¼šè°ƒç”¨ä»»ä½•å¤–éƒ¨API
    """
    
    if not enable_claude:
        return """âŒ Claudeåˆ†æåŠŸèƒ½æœªå¯ç”¨
        
è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ï¼Œéœ€è¦ï¼š
1. æ˜ç¡®è®¾ç½® enable_claude=True
2. é…ç½®Claude APIå¯†é’¥
3. ç¡®è®¤è¦ä½¿ç”¨å¤–éƒ¨APIæœåŠ¡

å¦‚éœ€å¯ç”¨ï¼Œè¯·ä½¿ç”¨ï¼š
experimental_claude_analysis(content="ä½ çš„å†…å®¹", enable_claude=True)
"""
    
    try:
        # æ£€æŸ¥Claudeé…ç½®
        claude_config_path = Path("v6_config/claude_config.json")
        if not claude_config_path.exists():
            return "âŒ Claudeé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆé…ç½®Claude API"
        
        with open(claude_config_path, 'r', encoding='utf-8') as f:
            claude_config = json.load(f)
        
        if not claude_config.get("claude_api", {}).get("enabled", False):
            return "âŒ Claude APIæœªå¯ç”¨ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨"
        
        api_key = claude_config.get("claude_api", {}).get("api_key", "")
        if not api_key:
            return "âŒ Claude APIå¯†é’¥æœªé…ç½®"
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„Claude APIè°ƒç”¨é€»è¾‘
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
        return f"""ğŸ§ª **Claudeåˆ†æç»“æœ** (å®éªŒæ€§)

ğŸ“ **åˆ†æå†…å®¹**: {content[:100]}...
ğŸ¯ **åˆ†æç±»å‹**: {analysis_type}
âš ï¸  **çŠ¶æ€**: å®éªŒæ€§åŠŸèƒ½ï¼ŒClaude APIè°ƒç”¨é€»è¾‘å¾…å®ç°

ğŸ’¡ **æç¤º**: è¿™ä¸ªåŠŸèƒ½ç›®å‰å¤„äºå¼€å‘é˜¶æ®µï¼Œå®é™…çš„Claude APIé›†æˆæ­£åœ¨å®Œå–„ä¸­ã€‚
"""
        
    except Exception as e:
        return f"âŒ Claudeåˆ†æå¤±è´¥: {str(e)}"

# ===== V6 åçˆ¬è™«å·¥å…· =====

@mcp.tool()
async def crawl_stealth(url: str, ctx: Context = None) -> str:
    """éšèº«çˆ¬å– - ä½¿ç”¨åæ£€æµ‹æŠ€æœ¯ç»•è¿‡åçˆ¬è™«æœºåˆ¶
    
    åŠŸèƒ½:
    - éšæœºUser Agentå’Œæµè§ˆå™¨æŒ‡çº¹
    - éšè—è‡ªåŠ¨åŒ–æ£€æµ‹ç‰¹å¾
    - éšæœºè§†çª—å¤§å°
    - åæ£€æµ‹æµè§ˆå™¨å‚æ•°
    """
    
    try:
        # å¯¼å…¥åæ£€æµ‹åŠŸèƒ½
        import sys
        sys.path.append('legacy/servers')
        from anti_detection import create_stealth_config
        
        # ä½¿ç”¨éšèº«é…ç½®
        browser_config = create_stealth_config()
        crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            word_count_threshold=50,
            delay_before_return_html=1  # ç¨å¾®å»¶è¿Ÿé¿å…æ£€æµ‹
        )
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=crawl_config)
            
            if result.success:
                # æ˜¾ç¤ºä¼ªè£…ä¿¡æ¯
                ua_info = browser_config.user_agent[:80] + "..." if len(browser_config.user_agent) > 80 else browser_config.user_agent
                viewport_info = f"{browser_config.viewport_width}x{browser_config.viewport_height}"
                
                response = f"ğŸ¥· **éšèº«çˆ¬å–æˆåŠŸ**\n\n"
                response += f"ğŸ”— **URL**: {url}\n"
                response += f"ğŸ­ **ä¼ªè£…UA**: {ua_info}\n"
                response += f"ğŸ“± **è§†çª—**: {viewport_info}\n"
                response += f"ğŸ›¡ï¸ **åæ£€æµ‹**: å·²å¯ç”¨\n"
                response += f"ğŸ“„ **æ ‡é¢˜**: {result.metadata.get('title', 'æœªçŸ¥')}\n"
                response += f"ğŸ“Š **å­—æ•°**: {len(result.markdown.split()) if result.markdown else 0}\n"
                response += f"\nğŸ“ **å†…å®¹**:\n\n{result.markdown[:3000]}..."
                
                return response
            else:
                return f"âŒ éšèº«çˆ¬å–å¤±è´¥: {result.error_message}"
                
    except ImportError:
        return "âŒ åæ£€æµ‹æ¨¡å—æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥legacy/servers/anti_detection.pyæ˜¯å¦å­˜åœ¨"
    except Exception as e:
        return f"âŒ éšèº«çˆ¬å–å‡ºé”™: {str(e)}"

@mcp.tool()
async def crawl_with_geolocation(url: str, location: str = "random", ctx: Context = None) -> str:
    """åœ°ç†ä½ç½®ä¼ªè£…çˆ¬å– - æ¨¡æ‹Ÿä¸åŒåœ°ç†ä½ç½®è®¿é—®ï¼Œç»•è¿‡åœ°åŒºé™åˆ¶
    
    å‚æ•°:
    - url: ç›®æ ‡ç½‘é¡µURL
    - location: åœ°ç†ä½ç½® (random/newyork/london/tokyo/sydney/paris/berlin/toronto/singapore)
    
    åŠŸèƒ½:
    - ä¼ªè£…GPSåæ ‡
    - ç»•è¿‡åœ°åŒºå†…å®¹é™åˆ¶
    - éšæœºåœ°ç†ä½ç½®é€‰æ‹©
    """
    
    try:
        # å¯¼å…¥åœ°ç†ä½ç½®ä¼ªè£…åŠŸèƒ½
        import sys
        sys.path.append('legacy/servers')
        from anti_detection import create_geo_spoofed_config
        
        # åˆ›å»ºåœ°ç†ä½ç½®ä¼ªè£…é…ç½®
        browser_config, geo_config = create_geo_spoofed_config(location)
        crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            word_count_threshold=50
        )
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=crawl_config)
            
            if result.success:
                response = f"ğŸŒ **åœ°ç†ä½ç½®ä¼ªè£…çˆ¬å–æˆåŠŸ**\n\n"
                response += f"ğŸ”— **URL**: {url}\n"
                response += f"ğŸ“ **ä¼ªè£…ä½ç½®**: çº¬åº¦ {geo_config.latitude:.4f}, ç»åº¦ {geo_config.longitude:.4f}\n"
                response += f"ğŸ¯ **ç²¾åº¦**: {geo_config.accuracy:.1f}ç±³\n"
                response += f"ğŸ“„ **æ ‡é¢˜**: {result.metadata.get('title', 'æœªçŸ¥')}\n"
                response += f"ğŸ“Š **å­—æ•°**: {len(result.markdown.split()) if result.markdown else 0}\n"
                response += f"\nğŸ“ **å†…å®¹**:\n\n{result.markdown[:3000]}..."
                
                return response
            else:
                return f"âŒ åœ°ç†ä½ç½®ä¼ªè£…çˆ¬å–å¤±è´¥: {result.error_message}"
                
    except ImportError:
        return "âŒ åœ°ç†ä½ç½®ä¼ªè£…æ¨¡å—æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥legacy/servers/anti_detection.pyæ˜¯å¦å­˜åœ¨"
    except Exception as e:
        return f"âŒ åœ°ç†ä½ç½®ä¼ªè£…çˆ¬å–å‡ºé”™: {str(e)}"

@mcp.tool()
async def crawl_with_retry(url: str, max_retries: int = 3, ctx: Context = None) -> str:
    """é‡è¯•çˆ¬å– - æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼Œé€‚åˆä¸ç¨³å®šæˆ–æœ‰åçˆ¬è™«çš„ç½‘ç«™
    
    å‚æ•°:
    - url: ç›®æ ‡ç½‘é¡µURL
    - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•° (1-5)
    
    åŠŸèƒ½:
    - æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥
    - éšæœºå»¶è¿Ÿé¿å…æ£€æµ‹
    - è‡ªåŠ¨åˆ‡æ¢éšèº«æ¨¡å¼
    """
    
    try:
        # å¯¼å…¥é‡è¯•ç®¡ç†å™¨
        import sys
        sys.path.append('legacy/servers')
        from anti_detection import create_retry_manager, create_stealth_config
        
        retry_manager = create_retry_manager(max_retries)
        browser_config = create_stealth_config()  # ä½¿ç”¨éšèº«æ¨¡å¼æé«˜æˆåŠŸç‡
        crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            word_count_threshold=50
        )
        
        start_time = time.time()
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await retry_manager.execute_with_retry(crawler, url, crawl_config)
            
            elapsed_time = time.time() - start_time
            
            if result.success:
                response = f"ğŸ”„ **é‡è¯•çˆ¬å–æˆåŠŸ**\n\n"
                response += f"ğŸ”— **URL**: {url}\n"
                response += f"â±ï¸ **ç”¨æ—¶**: {elapsed_time:.2f}ç§’\n"
                response += f"ğŸ” **æœ€å¤§é‡è¯•**: {max_retries}æ¬¡\n"
                response += f"ğŸ›¡ï¸ **éšèº«æ¨¡å¼**: å·²å¯ç”¨\n"
                response += f"ğŸ“„ **æ ‡é¢˜**: {result.metadata.get('title', 'æœªçŸ¥')}\n"
                response += f"ğŸ“Š **å­—æ•°**: {len(result.markdown.split()) if result.markdown else 0}\n"
                response += f"\nğŸ“ **å†…å®¹**:\n\n{result.markdown[:3000]}..."
                
                return response
            else:
                return f"âŒ é‡è¯•çˆ¬å–å¤±è´¥: {result.error_message}"
                
    except ImportError:
        return "âŒ é‡è¯•ç®¡ç†æ¨¡å—æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥legacy/servers/anti_detection.pyæ˜¯å¦å­˜åœ¨"
    except Exception as e:
        return f"âŒ é‡è¯•çˆ¬å–å‡ºé”™: {str(e)}"

# ===== å…¼å®¹æ€§å·¥å…· - ç»§æ‰¿V5åŠŸèƒ½ =====

@mcp.tool()
async def crawl_with_intelligence(
    url: str,
    use_smart_analysis: bool = True,
    ctx: Context = None
) -> str:
    """æ™ºèƒ½ç½‘é¡µçˆ¬å– - çˆ¬å–æŒ‡å®šç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼
    
    å‚æ•°:
    - url: ç›®æ ‡ç½‘é¡µURL
    - use_smart_analysis: æ˜¯å¦å¯ç”¨æ™ºèƒ½åˆ†æä¼˜åŒ–çˆ¬å–ç­–ç•¥
    
    åŠŸèƒ½:
    - çˆ¬å–ç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸ºç»“æ„åŒ–Markdown
    - æ™ºèƒ½åˆ†æç½‘é¡µç±»å‹ï¼Œè‡ªåŠ¨è°ƒæ•´çˆ¬å–ç­–ç•¥
    - æ”¯æŒåŠ¨æ€å†…å®¹åŠ è½½çš„ç½‘é¡µ
    - æä¾›çˆ¬å–ç»“æœçš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    """
    
    try:
        # åˆ†æURLæ„å›¾
        if use_smart_analysis:
            intent = analyze_user_intent(f"çˆ¬å– {url}")
            
            # æ ¹æ®æ„å›¾è°ƒæ•´çˆ¬å–ç­–ç•¥
            browser_config = BrowserConfig(
                headless=True,
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            )
            
            crawl_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                word_count_threshold=50,
                delay_before_return_html=2 if intent.dynamic_content else 0
            )
        else:
            browser_config = BrowserConfig(headless=True)
            crawl_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
        
        # æ‰§è¡Œçˆ¬å–
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=crawl_config)
            
            if result.success:
                response = f"ğŸ•·ï¸ **V6 æ™ºèƒ½çˆ¬å–ç»“æœ**\n\n"
                response += f"ğŸ”— **URL**: {url}\n"
                response += f"ğŸ“„ **æ ‡é¢˜**: {result.metadata.get('title', 'æœªçŸ¥')}\n"
                response += f"ğŸ“Š **å­—æ•°**: {len(result.markdown.split()) if result.markdown else 0}\n"
                
                if use_smart_analysis:
                    response += f"ğŸ§  **æ™ºèƒ½åˆ†æ**: å·²å¯ç”¨\n"
                
                response += f"\nğŸ“ **å†…å®¹**:\n\n{result.markdown[:3000]}..."
                
                return response
            else:
                return f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}"
                
    except Exception as e:
        return f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {str(e)}"

# ===== å¯åŠ¨ä¿¡æ¯ =====

def show_v6_welcome():
    """æ˜¾ç¤ºV6å¯åŠ¨ä¿¡æ¯"""
    print("ğŸš€ Context Scraper V6 MCP Server")
    print("=" * 50)
    print("ğŸ” ä¸»è¦åŠŸèƒ½:")
    print("   - æ™ºèƒ½ç½‘é¡µçˆ¬å–å’Œå†…å®¹æå–")
    print("   - æœç´¢æŒ‡å—å’Œæœ€ä½³å®è·µ")
    print("   - æ„å›¾åˆ†æå’Œç­–ç•¥ä¼˜åŒ–")
    print("   - å®éªŒæ€§Claudeåˆ†æåŠŸèƒ½")
    print("=" * 50)
    print("ğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("   - smart_search_guide: æ™ºèƒ½æœç´¢æŒ‡å—")
    print("   - crawl_with_intelligence: æ™ºèƒ½ç½‘é¡µçˆ¬å–")
    print("   - crawl_stealth: éšèº«æ¨¡å¼çˆ¬å–")
    print("   - crawl_with_geolocation: åœ°ç†ä½ç½®ä¼ªè£…")
    print("   - crawl_with_retry: é‡è¯•æ¨¡å¼çˆ¬å–")
    print("   - experimental_claude_analysis: Claudeåˆ†æ")
    print()
    print("ğŸ” æœç´¢åŠŸèƒ½:")
    print("   ä½¿ç”¨ smart_search_guide è·å–æœç´¢æŒ‡å¯¼")
    print("   åˆ©ç”¨ç°æœ‰çˆ¬å–å·¥å…·å®ç°é«˜æ•ˆæœç´¢")
    print("=" * 50)

if __name__ == "__main__":
    show_v6_welcome()
